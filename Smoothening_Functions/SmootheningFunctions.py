    

# This file has been altered to fit the needs of Phd Candidate Eivind Hugaas
# In general, only very few original functions from Søren Heinze are used. However, the approach, using dictionaries
# and simply eliminating measurements containing noise, have been kept more or less the same.
# The original copyright of Heinze have been kept for now as some of his functions are still in use.
# This functional script is however rather messy and awaits a clean up. The original copyright text of Heinze have been kept seeing as some of his functions are still employed.


#"OBR_Noise_reducer" (v1.0)
#    Copyright 2016 Soren Heinze
#    soerenheinze <at> gmx <dot> de
#    5B1C 1897 560A EF50 F1EB 2579 2297 FAE4 D9B5 2A35
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.



# A small program to automatically detect outliers in analyzed OBR data.
# 
# This program was written for files generated by the OBR-measurement/analyzing
# program, as stated in the accompanying manual to this program, in connection
# with the OBR_analyzer_simple.py program.
# 
# It is assumed that just ONE property if interest (e.g. either strain OR 
# temperature) is saved in each file.
# In this case the left column should contain the positional values and 
# the right column the values for the property of interest.
# 
# Before the table with the actual values some other data is present in the 
# analysis-file.
# However the table starts usually with some keywords directly followed
# by the data.
# In the functions below I use the following keywords: "Length", "microstrain", 
# "(deg C)", "Spectral". This should cover the most common cases. If this is not
# the case, extra keywords can simply be added in these functions.
# 
# It is also assumed that neither the central position of the area of interest, 
# or other (virtual) strain gauge parameters changed during the analysis of 
# the OBR data.
# In general (and especially ) this should be the case.
# 
# In addition the following assumptions are made regarding the data
# 0. the data is mainly "good" and has some outliers
# 1. the data is sectionwise "flat"
# 2. three (or more) "flat points" are considered to be one section
# 3. linear relation between flat sections
# 4. equally spaced points in x-direction
# 5. I actually need TWO flat sections to be able to "interpolate"
# 
# Due to rounding errors of the OBR analysis program, assumption 4 is not 
# entirely true. But the error in position is far below the measurement
# resolution and this program compensates for such eventualities.
# 
# Since some outliers escape this algorithm, this program creates (initially) 
# identical noise reduced files in two locations. So that the user can 
# manually correct the remaining outliers.
# See also the accompanying manual to this program.
# 
# For higher user friendlyness to detect the remaining outliers, each outlier 
# corrected dataset is automatically plotted and saved as a PNG file.
# This requires the installation of matplotlib.
# See the relevant document regarding this issue.
# 
# ATTENTION: It is assumed that the OBR_Filesorter.py program was used to sort 
# the analysis files. This includes the creation of certain folders that are 
# assumed to exist throughout this program. 
# If not these folders need to be created. 
# See also the accompanying manual to this program.
# 
# The parameters: maximum_difference_between_points (to determine how flat a 
# "flat section" is (see also point 1 above) and 
# maximum_difference_for_interpolation (as maximum allowed deviation from the 
# linear line between two flat sections to still count as datapoint, see also
# point 3 above) worked out fine for me, but probably need to be adjusted.
# 
# The main part of this file is the definition of functions. The actual 
# execution of these functions (a.k.a. running the program) takes place at the 
# end of this file.
# 
# ATTENTION: It is assumed that the running reference approach was used to 
# analyze the OBR rawdata.
# This means that from measurement two onwards, each file was analyzed
# once with the last measurement as reference and once with the second to last
# measurement as reference file. The filenames of the latter analysis should 
# contain "_older_reference" to indicate this.
# This is the case of the OBR_analyzer_simple.py program was used for the 
# analysis of the OBR rawdata.

# On older computers it may take a while before all modules are  loaded.
# Impatient users may think the program crashed. Hence, this message.

# print "Loading modules (this may take some seconds) ..."

from scipy.interpolate import Rbf
from copy import deepcopy
import matplotlib.pyplot as plt
import os
import collections
# shutil is here for using the correct operative specific move commands.
import shutil
from math import log
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d.axes3d import Axes3D, get_test_data
from matplotlib import cm
import numpy as np
from os import listdir
from os.path import isfile, join

# This function reads the data from a file and returns a list that
# contains tuples which contain the data in the correct order.

# The correct order is important to find the "flat" sections.

def Difference_over_cycles3(List_Of_Measurements,Length,Data={}):
    Dict=deepcopy(Data)

    #Given a list of measurements, typically at different loadlevels such as 13,23,33... then give give a Strain for each measurement on a given length given the full measurementdicitonary. 

    StrainAtLength=[]
    for measurement in List_Of_Measurements:
        measurementdict=Dict[measurement]
        StrainAtLength.append(measurementdict[smoothening().find_nearest(measurementdict, Length)])

    return StrainAtLength

class smoothening:
    
    def __init__(self):
        Check="OK"
		

    def data_from_one_file(self,In_File=''):
        # This list will be returned and contains just the positional data
        # and the values of the property of interest as tuples.
        data = []
        # Before the actual table with the data some metadata is written
        # into the files.
        # Right before the data a line with a keyword (e.g. "Length") appears.
        # This will sett data_is_here to True. As long as this is not the 
        # case, nothin will be written into the data-list defined above.
        data_is_here = False
        # Open the file with the data ...
        with open(In_File, 'r') as f:
            # ... scan through it line for line ...
            for line in f:
                # ... and if data_is_here is set to True ...
                if data_is_here:
                    # ... split this line at the "\t" (raw will be a list
                    # that contains the position as first value and the 
                    # value of the property of interest as second value but
                    # both will be strings and NOT numbers), ...
                    raw = line.strip().split('\t')
                    # ... convert the strings to numbers, ...
                    a = float(raw[0])
                    b = float(raw[1])
                    # ... create a tuple with these numbers ...
                    one_coord = (a, b)
                    # ... and append them to data; ...
                    data.append(one_coord)
                # ... but if data_is_here is still False, check if one of the 
                # keywords appears, and if this is the case ...
                elif 'Length' in line or 'microstrain' in line \
                                     or '(deg C)' in line or 'Spectral' in line:
                    # ... set data_is_here to True.
                    data_is_here = True

        #Make Data a Dict, such as all other data:
        DataDict={}
        for point in data:
            smoothening().transfer_point_into_dict(The_Dict=DataDict,Point=point)

        # When the end of the file is reached, return the data.
        #print(data)
        return DataDict



    # This function reads the data from the previous measurement and the 
    # measurement with an older reference. It returns a dict (!).
    # I don't need the correct order later, but the ability to access
    # the y-values to given x-values directly.
    # 
    # This is very much like data_from_one_file() with some exceptions
    # I mention below.
    def Write_Measurement_Dictionary_to_Files(self,Measurement_Dictionary={},Write_Location='',Name_Base=''):
        Dict_to_write=deepcopy(Measurement_Dictionary)

        for Measurement, Data in Dict_to_write.items():
            
            Write_file=os.path.join(Write_Location,Name_Base+(str(Measurement)).zfill(4)+'_Lower_Smooth.txt')
     
            if not os.path.exists(os.path.dirname(Write_file)):
                os.makedirs(os.path.dirname(Write_file))
                    
            with open(Write_file, 'w') as f:
                f.writelines('Length (m)\tStrain (microstrain)\t\n')
                for Length, Strain in Data.items():
                    Line=str(Length)+'\t'+str(Strain)+'\n'
                    f.writelines(Line)
                    
    def get_extra_data(self,other_reference_measurement, previous_measurement):
        # In contrast to data_from_one_file() I create here dictionaries and
        # NOT lists.
        other_reference = {}
        previous = {}

        data_is_here = False
        # First, get the data from the file with the second to last measurement
        # as reference.
        with open(other_reference_measurement, 'r') as f:
            for line in f:
                if data_is_here:
                    raw = line.strip().split('\t')
                    a = float(raw[0])
                    b = float(raw[1])
                    # Here is another diference to data_from_one_file(). Since
                    # 
                    other_reference[a] = b
                elif 'Length' in line or 'microstrain' in line \
                                     or '(deg C)' in line or 'Spectral' in line:
                    data_is_here = True

        # Don't forget to set data_is_here back to False because otherwise all the 
        # metadata will be read and that will lead to errors down the line.
        data_is_here = False

        # Second, get the data from the previous measurement.
        with open(previous_measurement, 'r') as f:
            for line in f:
                if data_is_here:
                    raw = line.strip().split('\t')
                    a = float(raw[0])
                    b = float(raw[1])
                    previous[a] = b
                elif 'Length' in line or 'microstrain' in line \
                                     or '(deg C)' in line or 'Spectral' in line:
                    data_is_here = True

        return (other_reference, previous)



    # This function can checks if the two points to the left/right to points 
    # 
    # data is the list that contains the datapoints as tuples.
    # which_point is the position of the datapoint in question in this list;
    # NOT the "x"-value!
    # ATTENTION: which_point must be >= 2!
    # ATTENTION 2: remember that counting starts at ZERO!
    # ATTENTION 3: The first two points are NOT to be evaluated.
    # See also find_flat_sections().d
    # I could consider those points too, but that would require some
    # if-statements I deem not necessary.
    def check_to_the_side(self,Data={}, Which_Point=3000000, Maximum_Difference_Between_Points=1000000, Which_Direction = 'right'):
        data=deepcopy(Data)
        
        # one_step is used to check one (or two) step(s) to the left/right.
        # In case I need to check to the right one_step needs to be positive ...
        one_step = 1
        # ... in case I need to check to the left, one_step needs to be negative.
        if Which_Direction == 'left':
            one_step = -1

        # Don't forget, that data is a list, that contains tuples.
        value = data[Which_Point][1]
        first_side_value = data[(Which_Point + one_step)][1]
        second_side_value = data[(Which_Point + 2*one_step)][1]

        # The values have to be in "one line" and the difference
        # between them shall not be greater than a given value.
        difference_point_to_first = abs(value - first_side_value)
        difference_first_to_second = abs(first_side_value - second_side_value)

        # Just if all three points are in one line (with the given allowed
        # difference between two points), this point is considered to be
        # in a straight section.
        if difference_point_to_first <= Maximum_Difference_Between_Points and difference_first_to_second <= Maximum_Difference_Between_Points:
            return True
        else:
            return False



    # This is more or less the same as check_to_the_side() but here I check the 
    # point to left and the point to the right.
    def check_middle(self,Data={}, Which_Point=10000, Maximum_Difference_Between_Points=10000000000):
        data=deepcopy(Data)
        value = data[Which_Point][1]
        left_side_value = data[(Which_Point - 1)][1]
        right_side_value = data[(Which_Point + 1)][1]

        difference_to_left = abs(value - left_side_value)
        difference_to_right = abs(value - right_side_value)

        if difference_to_left <= Maximum_Difference_Between_Points and difference_to_right <= Maximum_Difference_Between_Points:
            return True
        else:
            return False



    # This is just a summary of the check_to_the_side() and check_middle()
    # to have just one line in later sections of the program.
    def belongs_to_straight_section(self,Data={}, Which_Point=300000, Maximum_Difference_Between_Points=100000000):
        data=deepcopy(Data)
        if not smoothening().check_to_the_side(Data=data, Which_Point=Which_Point, Maximum_Difference_Between_Points=Maximum_Difference_Between_Points, Which_Direction='right') and not smoothening().check_to_the_side(Data=data, Which_Point=Which_Point, Maximum_Difference_Between_Points=Maximum_Difference_Between_Points, Which_Direction='left') and not smoothening().check_middle(Data=data, Which_Point=Which_Point, Maximum_Difference_Between_Points=Maximum_Difference_Between_Points):
            return False
        else:
            return True



    # I need to transfer quite often data into dicts throughout this program.
    # Thus I created just function for that to keep the functions where this 
    # is done more orderly.
    # point must be the tupel (x_value, y_value).
    def transfer_point_into_dict(self,The_Dict={}, Point=[]):
        data=The_Dict
        
        x_value = Point[0]
        y_value = Point[1]
        data[x_value] = y_value

    def transform_list_to_dict(self,Data=[]):
        the_dict={}
        List=Data
        for Entry in List:
            smoothening().transfer_point_into_dict(The_Dict=the_dict, Point=Entry)
        return the_dict



    #Due to standardazing of format to dict, some of the older functions has to have the following function.	

    def transform_dict_to_list(self,Data={}):
        Dictionary=deepcopy(Data)
        dictlist=[]
        for key, value in Dictionary.items():
            temp = [key,value]
            dictlist.append(temp)		

        return dictlist


    # This functions figures out if which of the datapoints in data are within a 
    # flat section (three points in one line, without outliers in between).
    # 
    # This function combines belongs_to_straight_section() and 
    # transfer_point_into_dict() to actually find the flat sections in the data 
    # and transfer it to the "these points are ok"-data.
    def find_flat_sections(self,Data={}, Maximum_Difference_Between_Points=10000):
        data=deepcopy(Data)
        # Create the containers for the real data and the outliers.
        # After this function is finished, real_data will contain so far just
        # the points in flat sections.
        # outliers contains all the remaining points. 
        # If outliers turn out to be real data at a later step, these will be 
        # transfered into real_data and deleted from outliers.
        real_data = {}
        outliers = {}
        # For EACH of the datapoints it will be checked if it belongs to
        # a flat section. If yes, it will be transfered into real_data, 
        # if not, into outliers.
        #
        # ATTENTION: DON'T EVALUATE THE FIRST TWO AND THE LAST TWO POINTS!
        # THE FUNCTIONS ABOVE CAN'T DO THAT!
        # With the data I have, this doesn't matter, since I usually have 
        # a lot of unnecessary data at the sides.
        if type(data) is dict:
            data=smoothening().transform_dict_to_list(Data=data)

        for i in range(2, (len(data) - 2)):
            # If a point belongs to a flat section ...
            if smoothening().belongs_to_straight_section(Data=data, Which_Point=i, Maximum_Difference_Between_Points=Maximum_Difference_Between_Points):
                # ... transfer it to real_data ...
                smoothening().transfer_point_into_dict(real_data, data[i])
            # ... otherwise, ...
            else:
                # ... transfer it to outliers.
                smoothening().transfer_point_into_dict(outliers, data[i])

        return (real_data, outliers)

    # Added by EH to take care of extreme increases in strain:
    def max_strain_increase(data, maxallowedstrainincrease):

        if type(data) is dict:
            data=transform_dict_to_list(data)
        realdata=[]
        unrealdata=[]
        realdatadict={}
        unrealdatadict={}

        for i in range(len(data)):
            if abs(data[i][1])<maxallowedstrainincrease:
                realdata.append(data[i])
            else:
                unrealdata.append(data[i])
        for point in realdata:
            transfer_point_into_dict(realdatadict,point)	

        for point in unrealdata:
            transfer_point_into_dict(unrealdatadict,point)		

        return realdatadict, unrealdatadict

    # To calculate the distance between points in x-direction.
    # I need this quite often so I wrote an extra function for that.
    # real_data is a dict.
    # ATTENTION: This function assumes that the points are equally spaced 
    # in x-direction. Hence it just 
    def calculate_constant_x_distance(self,Data={}):
        real_data=deepcopy(Data)
        x_values = sorted(real_data.keys())
        # Not all points are equally spaced in x-direction. See also the comment 
        # above and in find_start_and_end_of_unknown_sections().
        #  So I figure first out all possible distances between two adjecent points
        #  and put them into a set and into a list.
        # A set contains just unique values. So if a value shall be added to 
        # the set that already is in the set, it will not be added again.
        # However, in a list two identical values can be in there.
        # Afterwards, I check for each element in the set how often it appears
        # in the list and return the most common value.	
        set_of_all_distances = set()
        all_distances = []

        # First figure out all distances between the points and add them 
        # to the list and set ...
        for i in range(len(x_values) - 1):
            this_distance = x_values[i + 1] - x_values[i]
            set_of_all_distances.add(this_distance)
            all_distances.append(this_distance)

        # ... then find the distance that is most common.
        so_often = 0
        constant_x_distance = None
        for distance in set_of_all_distances:
            so_often_new = all_distances.count(distance)
            if so_often_new > so_often:
                so_often = so_often_new
                constant_x_distance = distance

        return constant_x_distance



    # This function checks the distance between all the points in the 
    # flat sections. If the distance between two points is bigger then the most 
    # common distance, these two points are the start- and end-point of the
    # unknown section.
    # See also the comment below.
    def find_start_and_end_of_unknown_sections(self,Data={}):
        real_data=deepcopy(Data)
        start_end_points = []
        # It is assumed that there actually are at least two flat sections.
        if len(real_data) == 0:
            # If this is not the case, the data is just noise.
            print("The data is just noise")
        elif len(real_data) == 1:
            # If this is not the case, the data is just noise.
            print("The data is just noise")	
        else:
            x_values = sorted(real_data.keys())
            # All points have (in theory) the same (characteristic) distance from 
            # each other in x-direction (position or length). That means that a gap
            # between two flat sections due to an outlier is equivalent
            # to two points NOT having this distance (in x-direction) to each 
            # other. Since I look just at the already determined flat section
            # data here, I should easily figure out these gaps by checking if 
            # two points have a different distance from each other than this
            # characteristic distance.
            # However, as mentioned above sometimes the positional value is 
            # NOT equidistant to the previous or following point due to rounding 
            # errors of the OBR measurement/analysis program. The error is very 
            # small and may be positive or negative. 
            # So simply checking if the distance between two points is not 
            # the characteristic distance may "find" gaps where there actually are
            # none.
            # BUT a gap contains at minimum width one datapoint. So two separate flat
            # sections are (in theory) at least two characteristic distances apart. 
            # That means checking if two points are more then 1 1/2 characteristic 
            # distances apart from each other will detect just real gaps between 
            # the flat sections.
            # 
            # This is an issue that comes up at several places throughout 
            # this whole program.
            minimum_distance_for_gap = smoothening().calculate_constant_x_distance(Data=real_data)
            minimum_distance_for_gap += minimum_distance_for_gap / 2

            # The last flat section can NOT have a "continuation" to the
            # "next" flat section (since it already is the last).
            # Hence it doesn't matter that I don't check the very last value.
            for i in range((len(x_values) - 1)):
                x_distance = abs(x_values[i] - x_values[i + 1])
                # Just if the distance between two points is significantly bigger 
                # then the most common distance between points it shall be 
                # recognized as a gap.
                if x_distance > minimum_distance_for_gap:
                    start = (x_values[i], real_data[x_values[i]])
                    end = (x_values[i + 1], real_data[x_values[i + 1]])
                    start_end_points.append((start, end))

        return start_end_points



    # This is to find the slope between two given points.
    # intervalls is a list that contains tuples. Each tuple containins two tuples, 
    # that contain the x- and y-coordinates.
    # See find_start_and_end_of_unknown_sections()
    def find_slopes_of_undefined_sections(self,Data={}):
        Intervall_List=deepcopy(Data)
        all_section_data = []
        # If there are no gaps in the data, I can not interpolate.
        # Most often this is because the data is just very good and does not 
        # have outliers.
        if len(Intervall_List) == 0:
            print ("No intervalls to interpolate between. No worries, probably it is just good data :) .")
        else:
            # For each gap ...
            for intervall in Intervall_List:
                # ... simply calculate the linear slope between two points.
                y_difference = intervall[1][1] - intervall[0][1]
                x_difference = intervall[1][0] - intervall[0][0]
                slope = y_difference / x_difference

                # In the end add the slope value to the start-/end-point tuples.
                # Since one can not append or add to tuples, I need to create
                # whole new tuples.
                all_section_data.append((intervall[0], intervall[1], slope))

        return all_section_data



    # This is just a summary of the find_start_and_end_of_unknown_sections() and
    # find_slopes_of_undefined_sections() to have just one line in reduce_noise().
    # Since this will run after the initial check for "flat"-sections it takes
    # this data as input.
    def find_undefined_sections(self,Data={}):
        data=deepcopy(Data)
        # First find the start- and end-point of the gaps between the flat 
        # sections ...
        undefined_sections = smoothening().find_start_and_end_of_unknown_sections(Data=data)
        # ... then calculate the slope of the line between these two points.
        undefined_sections_with_slopes = smoothening().find_slopes_of_undefined_sections(Data=undefined_sections)

        return undefined_sections_with_slopes



    # To know how many points there are in the intervall between two flat sections.
    # startpoint and endpoint are tuples.
    def so_many_points_in_intervall(self,Start_Point=789, End_Point=546456, Constant_X_Distance=56785):
        # I actually calculate NOT the number of points but how many "jumps"
        # of constant_x_distance are there between startpoint and endpoint.
        # The number of "jumps" must of course be reduced by one to get the
        # number of points.
        # 
        # Since int() returns everything before the comma, this is 
        # unaffected by the above mentioned rounding errors in the x-values
        # of the points.
        return int((End_Point[0] - Start_Point[0])/Constant_X_Distance - 1)



    # To calculate where the point should be if there is a linear 
    # slope between two flat sections.
    def calculate_point(self,Start=(), Constant_X_Distance=456356, Slope=34534, Which_Point=67867):
        point_x = Start[0] + Constant_X_Distance*(Which_Point + 1)
        # This is how the y-value should be.
        point_y = Slope*(Constant_X_Distance*(Which_Point + 1)) + Start[1]

        return (point_x, point_y)



    # Calculations sometimes come up with values that are not exactly as
    # measured, in this case the nearest value is most likely the one I'm
    # looking for and shall be used.
    # smoothening().find_nearest() searches the whole dict for the value I'm looking for.
    def find_nearest(self,datadict={}, datalist=[], X_Value_To_Be_Found=234):
        
        if datalist!=[]:
            x_values=datalist
        else:
            dictionary=deepcopy(datadict)
            x_values = dictionary.keys()
        old_difference = 10000000000000000000000.0
        nearest_value = -50000000000000000000000.0
        
        # The keys in the dicts I use in this program are the x-values of the 
        # points.
        # This loop calculates for all points the difference to the x-value
        # I'm looking for but "remembers" just the smallest difference and the
        # x-value it belongs to. This will be returned once all values are checked.
        for x_value in x_values:
            difference = abs(x_value - X_Value_To_Be_Found)
            if difference < old_difference:
                old_difference = difference
                nearest_value = x_value

        return nearest_value



    # This compares the calculated y-value with the measured y-value
    # and returns True if the difference is <= maximum_difference_for_interpolation.
    # 
    # Since the many times mentioned rounding errors of the x-values, it also 
    # returns the x_value that shall be used when saving this point.
    
   
    def point_is_real(self,Outliers=[], Calculated_Point=[], Maximum_Difference_For_Interpolation=678678):
        # calculated_point contains the "perfect" x-values if all points have
        # absolutely the same distance.
        # Again due to the rounding errors, this may not be the real x-value
        # and thus will not be found in the outliers-dict. If this is the case ...
        try:
            x_value = Calculated_Point[0]
            real_point_y = Outliers[x_value]
        # ... find the point that is nearest to this "perfect" calculated position.
        # I can do it this way, since the rounding errors are small and the actual
        # point is always the nearest to this calculated "perfect" position.
        except KeyError:
            x_value = smoothening().find_nearest(Outliers, Calculated_Point[0])
            real_point_y = Outliers[x_value]

        # Calculate the difference between tha actual point and the expected
        # value if this point would have been on the line between flat sections.
        difference = abs(real_point_y - Calculated_Point[1])

        # If this difference is small enough ...
        if difference <= Maximum_Difference_For_Interpolation:
            # ... return True and what the (actual) x-value of this point is.
            return (True, x_value)
        else:
            return (False, None)



    # This function returns JUST the x-values of the points that turn out
    # to be NOT outliers!
    def find_more_points_in_one_intervall(self,Intervall_Data=[], Constant_X_Distance=8989, Outliers=[], Maximum_Difference_For_Interpolation=234234234):
        # A list that contains all x-values of the points that are figured out as 
        # real values.
        intervall_data=Intervall_Data
        real_points = []
        start = intervall_data[0]
        end = intervall_data[1]
        slope = intervall_data[2]

        # Calculate how many point there actually need to be in an intervall.
        points_in_intervall = smoothening().so_many_points_in_intervall(Start_Point=start, End_Point=end, Constant_X_Distance=Constant_X_Distance)
        for i in range(points_in_intervall):
            # Then for each of these assumed point calculate the x-values they 
            # would have if the point would be exactly on the line.
            assumed_point = smoothening().calculate_point(Start=start, Constant_X_Distance=Constant_X_Distance, Slope=slope, Which_Point=i)
            # Afterwards check if the actual point is in the vicinity of this
            # calculated point.
            # real_point is a tuple that contains True if the point is real and
            # the x-value of this point.
            real_point = smoothening().point_is_real(Outliers=Outliers, Calculated_Point=assumed_point, Maximum_Difference_For_Interpolation=Maximum_Difference_For_Interpolation)
            # If the point in question is NOT an outlier ...
            if real_point[0]:
                # ... append its x-value to the list ...
                real_points.append(real_point[1])

        # ... and in the end return the list with all the x-values of the points 
        # that turned out not to be outliers.
        return real_points



    # To check if real data points were dismissed initially because these
    # were not in a "flat"-section.
    # See also comment in reduce_noise().
    # Returned are the (probably expanded) real data dict and the 
    # (probably reduced) outlier dict.
    def check_if_point_is_on_line(self,undefined_sections, first_run_real_data, first_run_outliers, maximum_difference_for_interpolation):
        # This is probably not necessary but I don't like to mess with results
        # I already have, so I prefer to make a real copy.
        # more_real_data will be first_run_real_data with more points added to 
        # it; less_outliers will be first_run_outliers with points taken out of it.
        # Hence the names.
        more_real_data = deepcopy(first_run_real_data)
        less_outliers = deepcopy(first_run_outliers)

        # If there were no flat sections, the data is just noise.
        if len(more_real_data) == 0:
            print ("The data is just noise")
        else:
            # This is just to figure out (again) the distance in x-direction.
            constant_x_distance = smoothening().calculate_constant_x_distance(Data=more_real_data)
            # Now the real thing starts.
            for intervall in undefined_sections:
                # find_more_points_in_one_intervall() figures out points in the 
                # intervall (gap) that actually are NOT outliers and returns
                # the x-values of these points.
                # more_points are just the x-values!
                more_points = smoothening().find_more_points_in_one_intervall(intervall, constant_x_distance, less_outliers, maximum_difference_for_interpolation)

                # Then find the y-values to these x-values ...
                for x_value in more_points:
                    point = (x_value, less_outliers[x_value])
                    # ... add the (now as real recognized) points to the
                    # real data dict ...
                    smoothening().transfer_point_into_dict(The_Dict=more_real_data, Point=point)
                    # ... and at the end delete the point from the outliers.
                    del less_outliers[x_value]

        return (more_real_data, less_outliers)



    # Sometimes outliers actually are peaks that show up consistently.
    # If this is the case a check of the same data, obtained in a different way
    # may reveal these peaks.
    # This function does that.
    # 
    # other_reference_data is the same measurement but evaluated with a 
    # reference two measurements back (and not just one)
    # previous_data is the measurement from before evaluated in the regular way.
    # If the supposed outlier is real it's value, added to previous_data should 
    # add upp to other_reference_data.
    # 
    # ATTENTION: It is possible that noise from all the data compared in this 
    # function adds up so that an outlier is actually seen as real data.
    # In a testcase this happened approx. 5 times in 50.000 datapoints.
    # But these points are "classical" outliers and will be detected in the 
    # very last step.
    def check_other_data_for_peaks(self,second_run_real_data, second_run_outliers, other_reference_data, previous_data, maximum_difference_between_points):
        more_real_data = deepcopy(second_run_real_data)
        less_outliers = deepcopy(second_run_outliers)
        #print(less_outliers.keys())
        # .keys() returns just the x-values of the points.
        for outlier_x in list(less_outliers):# less_outliers.keys():
            # The same issue that appears due to rounding errors. 
            # See also e.g. comment in point_is_real() since there the same 
            # try ... except is implemented due to the same reason.
            try:
                calculated = less_outliers[outlier_x] + previous_data[outlier_x]
            except KeyError:
                # I don't need to "correct" for the values in less_outliers since
                # I take the value from there. 
                correct_x_value = smoothening().find_nearest(previous_data, outlier_x)
                calculated = less_outliers[outlier_x] + previous_data[correct_x_value]

            try:
                difference = abs(calculated - other_reference_data[outlier_x])
            except KeyError:
                correct_x_value = smoothening().find_nearest(other_reference_data, outlier_x)
                difference = abs(calculated - other_reference_data[correct_x_value])

            if difference <= maximum_difference_between_points:
                real_point = (outlier_x, less_outliers[outlier_x])
                transfer_point_into_dict(more_real_data, real_point)
                del less_outliers[outlier_x]

        return (more_real_data, less_outliers)



    # For adding up the data I need to fill in values. 
    # Here it is assumed that all remaining outliers actually are outliers, thus 
    # I don't delete any longer from the outlier dict.
    def interpolate_values(self,undefined_sections, last_run_real_data):
        even_more_real_data = deepcopy(last_run_real_data)
        constant_x_distance = smoothening().calculate_constant_x_distance(Data=even_more_real_data)

        for intervall_data in undefined_sections:

            start = intervall_data[0]
            end = intervall_data[1]
            slope = intervall_data[2]

            # Calculate how many point there actually need to be in an intervall.
            points_in_intervall = smoothening().so_many_points_in_intervall(Start_Point=start, End_Point=end, Constant_X_Distance=constant_x_distance)
            for i in range(points_in_intervall):
                # Then calculate for each of these points the value it should have
                # on the slope.
                assumed_point = smoothening().calculate_point(Start=start, Constant_X_Distance=constant_x_distance, Slope=slope, Which_Point=i)

                # Find the nearest point to that calculated point. 
                nearest_point = smoothening().find_nearest(Data=even_more_real_data, X_Value_To_Be_Found=assumed_point[0])
                difference = abs(assumed_point[0] - nearest_point)
                # This method interpolates all points between two flat sections.
                # But in the steps before I already added points in such gaps
                # if these turned out not to be outliers.
                # 
                # As mentioned already several times, do rounding errors occur
                # in the x-values. So the constant_x_distance is not as constant 
                # in real data as I assumed. 
                # 
                # Hence if I just search for assumed_point[0] to be in 
                # even_more_real_data I may add points which are already in there 
                # but which are a bit longer/lesser away from the last point than 
                # constant_x_distance. 
                # 
                # However, IF the point is already in even_more_real_data (so if it 
                # was recognized as a real point in a previous step), its x-value
                # is very near assumed_point[0] since these rounding errors are 
                # small. It is in any case nearer then constant_x_distance/2.
                # 
                # Thus if the nearest point to the assumed_point is farther away 
                # then this distance, assumed_point must be added to the real
                # data since there is no point at this position, yet. 
                if difference >= constant_x_distance/2:
                    smoothening().transfer_point_into_dict(The_Dict=even_more_real_data, Point=assumed_point)

        return even_more_real_data



    # There can be data before the first or after the last flat section. 
    # This may be noise, or real data with a steep slope (thus not "flat"). 
    # I can NOT interpolate in this case. Thus I just fill up with the 
    # original value.
    # original_data is a list, last_run_real_data is a dict.
    def fill_up_the_data(self,original_data, last_run_real_data):

        if type(original_data) is dict:
            original_data=smoothening().transform_dict_to_list(Data=original_data)

        final_real_data = deepcopy(last_run_real_data)
        constant_x_distance = smoothening().calculate_constant_x_distance(Data=last_run_real_data)

        # Yes, here I go through the complete original data.
        for x, y in original_data:
            if x not in final_real_data:
                nearest_x = smoothening().find_nearest(Data=last_run_real_data, X_Value_To_Be_Found=x)
                # See comment in interpolate_values why I use 
                # constant_x_distance/2 here.
                if abs(nearest_x - x) >= constant_x_distance/6:
                    final_real_data[x] = y

        return final_real_data



    # To write the noise reduced data into a file.
    # Since here the order is important again, final_data needs to be a list, 
    # that contains the tuples with coordinates.
    def write_to_file(outfile, final_data):
        with open(outfile, 'w') as f:
            for x, y in final_data:
                f.write('%s\t%s\n' % (x, y))



    # This function makes pictures of each data-set that shall help me with 
    # manually figuring out the last remaining outliers that are not detected
    # by this program.
    def plot_data(self,original_data, reduced_noise_data, save_file):
        with_noise = []
        without_noise = []

        if type(original_data) is dict:
            original_data=transform_dict_to_list(original_data)	

        for x, y in original_data:
            with_noise.append(y)
        for x, y in reduced_noise_data:
            without_noise.append(y)

        # If pyplt gets just a list, it fills up the x-values automatically.
        # I don't really care about the absolute x-position here, since I just 
        # need to compare the y-values at the same position.
        # just need a comparison so I 
        plt.plot(with_noise, 'ro', without_noise, 'bs')
        plt.plot(with_noise, 'ro')
        plt.plot(without_noise, 'bs')
        #plt.plot(with_noise, 'ro', without_noise)
        #plt.plot(with_noise, 'ro')
        #plt.plot(without_noise, 'bs')	
        plt.title('RED = with noise, BLUE = withOUT noise')
        # Save the plot


        plt.savefig(save_file)
        # Clear the plot
        plt.clf()


    # One function that does the stuff above.
    # ATTENTION: THIS WORKS JUST FOR DATA THAT HAS AN AVAILABLE
    # PREVIOUS MEASUREMENT AND A FILE WITH ANOTHER REFERENCE!
    # 
    # Hence this can NOT be used for the very first measurement!!!

    def plot_tuple(self,data):
        testList = data
        testList2 = [(elem1, elem2) for elem1, elem2 in testList]
        zip(*testList2)
        plt.scatter(*zip(*testList2))
        plt.show()		

    def SmoothenMeasurementsOverLength(self,Data={},Measurements_To_Smoothen=[],Max_Difference=456456,Maximum_Difference_Between_Points=456456,Maximum_Difference_For_Interpolation=456456):
        allfilesdictwithsmoothedsinglefiles=deepcopy(Data)
        data=deepcopy(Data)
        for measurement in Measurements_To_Smoothen:
            Measurement=data[measurement]
            MeasurementWithoutNoise=smoothening().ReduceNoiseOnSingleMeasurement(Measurement_Dict=Measurement,Max_Difference=Max_Difference,Maximum_Difference_Between_Points=Maximum_Difference_Between_Points,Maximum_Difference_For_Interpolation=Maximum_Difference_For_Interpolation)
            allfilesdictwithsmoothedsinglefiles[measurement]=MeasurementWithoutNoise

        return 	allfilesdictwithsmoothedsinglefiles



    def ReduceNoiseOnSingleMeasurement(self,Measurement_Dict={},Max_Difference=100000,Maximum_Difference_Between_Points=1000000,Maximum_Difference_For_Interpolation=1000000): #Data is {length; value}
        data=deepcopy(Measurement_Dict)
        
        # Say that nr. 5 is the measurement file in question

        # First read the data from the actual analysis file to be checked for 
        # outliers.
        # This will be the file with the last measurement as reference.

        #E01_fibre 1_0005_Lower.txt input, returns Dict.

        #plot_tuple(data)

        # Second get the data from the same measurement but with the second to last 
        # measurement as reference and also the data from the previous analysis.
        # Both are needed to check if an outlier probably is real data and just looks
        # like an outlier if just the data for this one analysis is taken into account.

        #other ref is the second to last ref as ref, this can be fixed ffs. previous measurement is E01_fibre 1_0004_Lower.txt


        # Second added (EH):
        Thresholded_data={}
        Threshold_data_outliers={}
        constant_x_distance=smoothening().calculate_constant_x_distance(Data=data)
        for key, value in data.items():
            if abs(value)>Max_Difference:

                Threshold_data_outliers[key]=value
            else:
                Thresholded_data[key]=value
                
        zeroth_check=deepcopy(smoothening().makeconstantx(Dictionary=Thresholded_data,constantdiff=constant_x_distance))

        # Third, find the flat sections in this data ...
        first_check = smoothening().find_flat_sections(Data=zeroth_check, Maximum_Difference_Between_Points=Maximum_Difference_Between_Points)
        # ... and sort into as real assumed data (the points in the flat sections) ...

        first_run_real_data = first_check[0] #Dict

        # ... and possible outliers (all points NOT in flat sections).

        first_run_outliers = first_check[1] #Dict

        first_run_outliers.update(Threshold_data_outliers)

        # Fourth, figure out the start- and end-points of the non-flat sections
        # and what the slope of the line between these points is.
        undefined_sections = smoothening().find_undefined_sections(Data=first_run_real_data)

        # Fifth, check if points that were assumed to be outliers, because these were
        # not in a flat section, probably are on the line between the start- and
        # end-point between two flat sections (vulgo: in the gap).
        # The points can "jitter" a bit around the line in y-direction.
        # How big this "jittering" can be is determined by 
        # maximum_difference_for_interpolation.
        # 
        # I assume that "dynamic regions" (e.g steep strain gradients) probably 
        # also lead to higher "jitter" values without the value of the property of 
        # interest being an outlier. To take this into account, 
        # maximum_difference_for_interpolation is bigger then 
        # maximum_difference_between_points which determines how "flat" a flat
        # section needs to be.
        # 
        # So this check is more or less just checking if the y-value of a point
        # is not larger or smaller then point then a the value determined by the
        # expected line in the gap.
        # 
        # Formerly assumed outliers that turn out to be real data during this 
        # check are transferd from the outlier-dict to the real-data-dict.
        # and both (modified) dicts are returned by check_if_point_is_on_line() 
        # and become second_run_real_data and second_run_outliers below.

        second_check = smoothening().check_if_point_is_on_line(undefined_sections, first_run_real_data, first_run_outliers, Maximum_Difference_For_Interpolation)
        second_run_real_data = second_check[0]
        second_run_outliers = second_check[1]

        # Sixth, if outliers show up at the same position and with approx. the same
        # value in the measurement with the older reference, it is very likely that 
        # these are actually NOT outliers but real values. Here this will be 
        # checked to reduce the chance of falsely dismissing real data.
        # See also comment to check_other_data_for_peaks().
        # The dicts with the real data and the outliers will be changed accordingly
        # and returned by check_other_data_for_peaks().
        undefined_sections = smoothening().find_undefined_sections(second_run_real_data)
        fourth_run_real_data = smoothening().interpolate_values(undefined_sections, second_run_real_data)


        # EH added. But before that, go through the other data and check that its reasonable:

        # Seventh, when using the running reference method to obtain the strain 
        # data, I need to add up all the different measurements. This means that
        # if outliers are detected, these can not just be left out, but must be
        # substituted in a meaningful way. The substituted values will be all
        # on the the line between two flat sections.

        # Eigth, situations may occur in which I can not interpolate. In these cases
        # just fill in the original data.
        # See also comment to fill_up_the_data()

        #final_real_data = fill_up_the_data(data, second_run_real_data)

        # Ninth, since a dict is unsorted, I need to sort the data before writing to
        # file.
        #final_real_data=fourth_run_real_data

        final_real_data = smoothening().fill_up_the_data(data, fourth_run_real_data)


        sorted_x_values = sorted(final_real_data.keys())
        data_to_be_written = []



        for x in sorted_x_values:
            data_to_be_written.append((x, final_real_data[x]))

        # Eleventh, due to "lucky" add-up of noise data some noise-points are 
        # considered to be real points. A test on approx. 50.000 data-points has 
        # show that this is the case for ca. 0,02 per cent of the points. So I 
        # have to check the data anyway manually to get rid of these points.
        # I make pictures of each data-set that shall help me with that.

        dict_to_be_written=smoothening().transform_list_to_dict(Data=data_to_be_written)
        return dict_to_be_written

        #Finally, make a file with 1 and 0's that outlines where noise was detected.



        #write_to_file(outfile, data_to_be_written)


    # The very first file will NOT undergo the noise reducing algorithm.
    # See the definitions and comments in the above functions why this is the case.
    # Anyhow, of course this file is needed in the same format as the other noise 
    # reduced files (JUST the table with the values) and with a proper filename 
    # ("0001_noise_reduced.txt") in the correct folders.
    # But the raw analyzed file has additional data above the table and the 
    # filename is not correct. I take care of this here.

    def make_binary_noisefiles(self,original_data, noise_reduced_data):
        binarynoise=[]

        data=original_data
        data_to_be_written=noise_reduced_data

        if type(data) is dict:
            data=transform_dict_to_list(data)

        if type(data_to_be_written) is dict:
            data_to_be_written=transform_dict_to_list(data_to_be_written)	

        for i in range(len(data)):
            if abs(data[i][1]-data_to_be_written[i][1])<1.0:
                binarynoise.append((data[i][0],1))
            else:
                binarynoise.append((data[i][0],0))


        write_to_file(binarynoisefile, binarynoise)


    def make_correct_first_file(self,file_location, name_base, minimum_number):
        first_file_addendum = name_base+(str(minimum_number-1)).zfill(4)+'_Lower.txt'   #'0001_Lower.txt'
        first_infile = os.path.join(file_location, first_file_addendum)
        first_outfile = os.path.join(file_location, (str(minimum_number-1)).zfill(4)+'_noise_reduced.txt')#'0001_noise_reduced.txt')

        data = []
        # Don't add everything, JUST add sth. if it belongs to the table 
        # of values.
        add = False

        # Open the first file and extract the data.
        with open(first_infile, 'r') as f:
            for line in f:
                if 'Length' in line or 'microstrain' in line \
                                   or '(deg C)' in line or 'Spectral' in line:
                    add = True
                elif add:
                    data.append(line.strip())

        # Now write this to the correct outfile.
        with open(first_outfile, 'w') as f:
            for element in data:
                f.write('%s\n' % element)


    # A function that does everything to move all the files into the
    # correct folders.
    # ATTENTION: It is assumed that these folders exist.
    def move_files(self,file_location):
        # The raw analysis files are located in file_location.
        # However, this is just a subfolder and I need to move all the noise
        # reduced files into other folders that are "one level above" the
        # RAW-folder.
        folder_base = file_location.split('RAW')[0]

        # Under windows " \ " is used instead of " / " to separate subfolders
        # in the file location name. 
        # This gave me major headaches how to account for that since " \ " has a 
        # special meaning in strings under python.
        # However, doing it this way I managed to get things done.
        # Especially os.path.join() is very useful to get operative system 
        # dependend seperators correct.
        all_folders = os.listdir(folder_base)

        for folder_name in all_folders:
            if 'Noise_reduced_before_manual_correction' in folder_name:
                first_folder = os.path.join(folder_base, folder_name)
            elif 'Noise_reduced_after_manual_correction' in folder_name:
                second_folder = os.path.join(folder_base, folder_name)
            elif 'PNG' in folder_name:
                png_folder = os.path.join(folder_base, folder_name)
            elif 'Binary_noise_files' in folder_name:
                binary_files = os.path.join(folder_base, folder_name)		

        all_sorted_filenames = sorted(os.listdir(file_location))

        for filename in all_sorted_filenames:
            if "_noise_reduced" in filename:
                from_here = os.path.join(file_location, filename)
                to_there_1 = os.path.join(first_folder, filename)
                to_there_2 = os.path.join(second_folder, filename)
                shutil.copy2(from_here, to_there_1)
                shutil.move(from_here, to_there_2)
            elif "_binary_noise" in filename:
                from_here = os.path.join(file_location, filename)
                to_there = os.path.join(binary_files, filename)
                shutil.move(from_here, to_there)			
            # Since I never know if the filename ending is upper or lower case when
            # these files are created, I just convert the filename to all caps.
            elif ".PNG" in filename.upper():
                from_here = os.path.join(file_location, filename)
                to_there = os.path.join(png_folder, filename)
                shutil.move(from_here, to_there)





    ## ## ## ## ## Here the execution of the program starts. ## ## ## ## ##



    # That I can call main() from other modules with the correct parameters 
    # I decided to have file_location, name_base and maximum_number as parameters.
    # 
    # To have this however also as a standalone program, I ask for these 
    # parameters in the if-construct that is called when this program is called
    # on the shell.
    def main(self,file_location, name_base, minimum_number, maximum_number, maximum_difference_between_points, maximum_difference_for_interpolation, maxallowedstrainincrease):


        # ATTENTION: DON'T CHECK THE FIRST FILE!!! 
        # IT DOESN'T HAVE A PREVIOUS MEASUREMENT FILE AND THAT WILL CAUSE ERRORS!
        for i in range(minimum_number, (maximum_number + 1)):
            print("noise reducing file %s"%i)
            # Just generate for each file the correct filenames to work with.
            # The above mentioned OBR measurement/analysis program sets
            # the word "_Lower" automatically at the end of each filename.
            # If OBR_analyzer_simple.py program was used to automatically press
            # the buttons for analysis, the file with the second to last measurement
            # as reference file automatically has "_older_reference_Lower" at the 
            # end of the filename.
            # 
            # # zfill() adds zeros in front of the given string. 
            # i is a number, not a string. Thus I use str() to convert it to a 
            # string first.
            # 
            # Thanks to windows I need to jump through hoops to get the 
            # proper filenames.
            infile_addendum = name_base + str(i).zfill(4) + '_Lower.txt'
            infile = os.path.join(file_location, infile_addendum)
            outfile_addendum = str(i).zfill(4) + '_noise_reduced.txt'
            outfile = os.path.join(file_location, outfile_addendum)

            binarynoisefile_addendum = str(i).zfill(4) + '_binary_noise.txt'
            binarynoisefile = os.path.join(file_location, binarynoisefile_addendum)

            other_reference_measurement_addendum = name_base + \
                            str(i).zfill(4) + '_Lower_summed.txt'
            other_reference_measurement = os.path.join(file_location, \
                                                                   other_reference_measurement_addendum)
            previous_measurement_addendum = name_base + str((i-1)).zfill(4) + '_Lower.txt'

            previous_measurement = os.path.join(file_location, previous_measurement_addendum)
            plotfile_addendum = str(i).zfill(4) + '_plot.png'
            plotfile = os.path.join(file_location, plotfile_addendum)

            # Here the actual noise reduction takes place, see the 
            # function-definition(s) above.
            reduce_noise(infile, outfile, binarynoisefile, other_reference_measurement, previous_measurement, plotfile, maximum_difference_between_points, maximum_difference_for_interpolation, maxallowedstrainincrease)

        # When all files are corrected for outliers, ...
        # 
        # ... make a proper first file 
        # (see comment to make_correct_first_file()) ...
        make_correct_first_file(file_location, name_base, minimum_number)
        # ... and move the files into the correct folders.
        move_files(file_location)




    def Difference_over_cycles(self,List_Of_Measurements,Length,Data={}):
        Dict=deepcopy(Data)

        #Given a list of measurements, typically at different loadlevels such as 13,23,33... then give give a Strain for each measurement on a given length given the full measurementdicitonary. 

        StrainAtLength=[]
        for measurement in List_Of_Measurements:
            measurementdict=Dict[measurement]
            StrainAtLength.append(measurementdict[smoothening().find_nearest(measurementdict, Length)])

        return StrainAtLength
    
    def CombineDictionaries(self,Dictinonary_List=[]):
        myDict = defaultdict(dict)
        
        mySortedDict = defaultdict(dict)

        for Dict in Dictinonary_List:
            for MeasurementNumber, StrainField in Dict.items():   
                for Length, Strain in StrainField.items():
                    myDict[MeasurementNumber][Length] = Strain
        
        for MeasurementNumber, StrainField in myDict.items():
            for key in sorted(StrainField.keys()):
                mySortedDict[MeasurementNumber][key]=deepcopy(myDict[MeasurementNumber][key])
                
        return mySortedDict
    
    def makeconstantx(self,Dictionary={},constantdiff=0.003):
        nesteddict=True 
        
        smallestdiff=constantdiff
        try:
            mydicttest=deepcopy(Dictionary)
            for MeasurementNumber, StrainField in mydicttest.items():
                oldkey=sorted(StrainField.keys())[0]
        except:
            nesteddict=False
        
        if nesteddict:        
            constantxdict=defaultdict(dict)
            mydict=deepcopy(Dictionary)
            for MeasurementNumber, StrainField in mydict.items():
                
                oldkey=sorted(StrainField.keys())[0]    
                difflist=[]
                l=abs(sorted(StrainField.keys())[-1]-sorted(StrainField.keys())[0])
                startl=sorted(StrainField.keys())[0]
                endl=sorted(StrainField.keys())[-1]           
                datapoints=int(l/smallestdiff)
                newx = np.linspace(startl, endl, num=datapoints, endpoint=True)
                xp=list(StrainField.keys())
                fp=list(StrainField.values())
                newy=np.interp(newx, xp, fp)
                for i in range(len(newx)):
    
                    constantxdict[MeasurementNumber][newx[i]]=newy[i]
                    
        if not nesteddict:     
            constantxdict={}
            StrainField=deepcopy(Dictionary)
            oldkey=sorted(StrainField.keys())[0]
                
            difflist=[]
            l=abs(sorted(StrainField.keys())[-1]-sorted(StrainField.keys())[0])
            startl=sorted(StrainField.keys())[0]
            endl=sorted(StrainField.keys())[-1]
            datapoints=int(l/smallestdiff)
           
            newx = np.linspace(startl, endl, num=datapoints, endpoint=True)
            xp=list(StrainField.keys())
            fp=list(StrainField.values())
            newy=np.interp(newx, xp, fp)
            for i in range(len(newx)):

                constantxdict[newx[i]]=newy[i]

        
        return constantxdict
 

    def noiseoveronemeasurement(self,data={},Max_Difference=100000000000000000.):
        dictionary=deepcopy(data)
        
        diffdictonlynoise=defaultdict(dict)
        
        for cycle, ramp in dictionary.items():
            firsttime=True
            for value, measurement in ramp.items():
                firsttime=True
                for length, strain in measurement.items():
                    if abs(strain)>Max_Difference:
                        if firsttime:
                            diffdictonlynoise[cycle][value]={}
                        diffdictonlynoise[cycle][value][length]=1.0
                        firsttime=False
        return diffdictonlynoise

    def noiseoverloadsteps(self,data={},Max_Difference_Between_Load_Steps=100000000000.):
    
        dictionary=deepcopy(data)
        
        dispdiffdictonlynoise=defaultdict(dict)

                        
        for cycle, ramp in dictionary.items():
            firsttime=True
            values=ramp.keys()
            listvalues=list(values)
            sortedvalues=sorted(list(listvalues))
            pastvalue=sortedvalues[0]
            
            for value in sortedvalues:
                firsttime=True
                currentmeasurement=deepcopy(dictionary[cycle][value])
                pastmeasurement=deepcopy(dictionary[cycle][pastvalue])
               
                pastvalue=value
               
                currentlength,currentstrain=zip(*list(currentmeasurement.items()))
                currentlength,currentstrain=np.array(currentlength),np.array(currentstrain)
                
                pastlength,paststrain=zip(*list(pastmeasurement.items()))
                pastlength,paststrain=np.array(pastlength),np.array(paststrain)
                
                difference=currentstrain-paststrain

                for t in range(len(currentlength)):
                    diff=difference[t]
                    length=currentlength[t]
                    if abs(diff)>Max_Difference_Between_Load_Steps:
                        if firsttime:
                            dispdiffdictonlynoise[cycle][value]={}
                        dispdiffdictonlynoise[cycle][value][length]=1.0
                        firsttime=False                

        return dispdiffdictonlynoise

    def noiseovercycles(self,data={},Max_Difference_Between_Cycles=43214321431):
        dictionary=deepcopy(data)

        cyclediffdictonlynoise=defaultdict(dict)
        
        cycles=dictionary.keys()
        listcycles=list(cycles)
        sortedcycles=sorted(list(listcycles))        
        
        pastcycle=0
        
        for cycle in sortedcycles:
            firsttime=True
            values=dictionary[cycle].keys()
            listvalues=list(values)
            sortedvalues=sorted(list(listvalues))
            
            pastvalues=dictionary[pastcycle].keys()
            pastlistvalues=list(pastvalues)
            pastsortedvalues=sorted(list(pastlistvalues))            
            
            for value in sortedvalues:
                firsttime=True
                
                nearestpastvalue=min(pastsortedvalues, key=lambda x:abs(float(x)-float(value)))
                
                print(nearestpastvalue)

                currentmeasurement=deepcopy(dictionary[cycle][value])
                pastmeasurement=deepcopy(dictionary[pastcycle][nearestpastvalue])
               
                currentlength,currentstrain=zip(*list(currentmeasurement.items()))
                currentlength,currentstrain=np.array(currentlength),np.array(currentstrain)
                
                pastlength,paststrain=zip(*list(pastmeasurement.items()))
                pastlength,paststrain=np.array(pastlength),np.array(paststrain)
                
                difference=currentstrain-paststrain

                for t in range(len(currentlength)):
                    diff=difference[t]
                    length=currentlength[t]
                    if abs(diff)>Max_Difference_Between_Cycles:
                        if firsttime:
                            cyclediffdictonlynoise[cycle][value]={}
                        cyclediffdictonlynoise[cycle][value][length]=1.0
                        firsttime=False                  
        
            pastcycle=cycle


        return cyclediffdictonlynoise

    def referencemetadatalist(self,metafile=r'C:\Users\eivinhug\OneDrive - NTNU\PhD\Testing\Laminate_F\OBR_Files\MeasurementMetadata.txt'):

        # This list will be returned and contains just the positional data
        # and the values of the property of interest as tuples.
        data = []
        cycles = []
        In_File=metafile
        # Before the actual table with the data some metadata is written
        # into the files.
        # Right before the data a line with a keyword (e.g. "Length") appears.
        # This will sett data_is_here to True. As long as this is not the 
        # case, nothin will be written into the data-list defined above.
        data_is_here = False
        # Open the file with the data ...
        with open(In_File, 'r') as f:
            # ... scan through it line for line ...
            for line in f:
                # ... and if data_is_here is set to True ...
                if data_is_here:
                    # ... split this line at the "\t" (raw will be a list
                    # that contains the position as first value and the 
                    # value of the property of interest as second value but
                    # both will be strings and NOT numbers), ...
                    raw = line.strip().split('\t')
                    # ... convert the strings to numbers, ...
                    try:
                        a = int(raw[0])
                    except:
                        a = float(raw[0])                    
                    b = float(raw[1])
                    try:
                        c = int(raw[2])
                    except:
                        c = float(raw[2])
                    # ... create a tuple with these numbers ...
                    one_coord = (c, a, b)
                    # ... and append them to data; ...
                    try:
                        d=raw[3]
                    except:
                        d=''
                    if d=='Zero strain reference':
                        print("Measurement %s is zero strain reference at displacement %s" % (str(a),str(b)))
                        cycles.append(c)
                        data.append(one_coord) 
                    if "Use" in d:
                        print("%s instead of %s" % (str(d),a))
                        cycles.append(c)
                        a=int(d.split(" ")[1])
                        data.append((c, a, b))                
                    if d=="Skip":
                        print("Skipping measurement %s" % (str(a)))
                    if not d and b!=0.0:
                        cycles.append(c)
                        data.append(one_coord)
                # ... but if data_is_here is still False, check if one of the 
                # keywords appears, and if this is the case ...
                elif 'nr.' in line or 'dis' in line or 'Cycle' in line or 'type' in line or 'Comment' in line or 'Fiber number:' in line:
                    # ... set data_is_here to True.
                    data_is_here = True
        return data  



    def interpolatetovalue(self,dictionary={},values=[],function='linear',onlyreturnthevalues=False):
        interpoldict=defaultdict(dict)
        for cycle,ramp in dictionary.items(): #goes through one ramping aka one set of values (disp or load)
            print(cycle)
            slist=[]
            llist=[]
            vlist=list(ramp.keys())
            tuplelist=[]
                             
            llist=list(ramp[vlist[0]].keys())  #makes a list of lengths, takes just the first value in valuelist
                
            for l in llist:                    #goes through the lengths one by one
                slist=[]
                for v in vlist:
                    slist.append(ramp[v][l])   #makes a strainlist on the 
                
                [float(v) for v in vlist]
                [float(s) for s in slist]
                [float(l) for l in llist]
              
                x,y=np.array(vlist),np.array(slist) 
                rbfi=Rbf(x, y,function=function)    
                valuelist=vlist+values
                valuelist=list(set(valuelist))
                valuelist.sort()
                strainlist=rbfi(np.array(valuelist))
                
                for i in range(len(strainlist)):
                    if onlyreturnthevalues:
                        if valuelist[i] in values:
                            tuplelist.append((cycle,valuelist[i],l,strainlist[i]))
                    
                    else:
                        tuplelist.append((cycle,valuelist[i],l,strainlist[i]))
                    
            
            sortedtuplelist=sorted(tuplelist, key=lambda x: x[1])    
            
            firsttime=True
            pastvalue=0.0
            for Tuple in sortedtuplelist:
                cycle=Tuple[0]
                value=Tuple[1]
                length=Tuple[2]
                strain=Tuple[3]
                if pastvalue!=value:
                    firsttime=True
                if firsttime:
                    interpoldict[cycle][value]={}
                firsttime=False
                interpoldict[cycle][value][length]=strain
                pastvalue=value    
       
        return deepcopy(interpoldict)
        
    def interpolatetovalueandcycle(self,dictionary={},values=[],cycles=[],function='linear',onlyreturnthevalues=False):
        interpoldict=defaultdict(dict)
        
        cyclelist=list(dictionary.keys())
        lllist=[]
        vvlist=[]
        cclist=[]
        sslist=[]
        for cycle,ramp in dictionary.items(): #goes through one ramping aka one set of values (disp or load)
            print(cycle)
            slist=[]
            llist=[]
            clist=[]
            vlist=list(ramp.keys())
            tuplelist=[]
                             
            llist=list(ramp[vlist[0]].keys())  #makes a list of lengths, takes just the first value in valuelist
                
            for l in llist:                    #goes through the lengths one by one
                for v in vlist:
                    sslist.append(ramp[v][l])   #makes a strainlist on the 
                    vvlist.append(v)
                    cclist.append(cycle)
                    lllist.append(l)
                    
                    
                    
        [float(v) for v in vvlist]
        [float(s) for s in sslist]
        [float(l) for l in lllist]
        [float(c) for c in cclist]
              
        x,y,z,d=np.array(cclist),np.array(vvlist),lllist,np.array(sslist)
        rbfi=Rbf(x, y, z, d, function=function)    
        
        cyclelist=cclist+cycles
        cyclelist=list(set(cyclelist))
        cyclelist.sort()
        
        valuelist=vlist+values
        valuelist=list(set(valuelist))
        valuelist.sort()
        
        firsttime=True
        pastvalue=0.0
        for cycle in cyclelist:
            for value in valuelist:
                for length in lengthlist:
                    if pastvalue!=value:
                        firsttime=True
                    if firsttime:
                        interpoldict[cycle][value]={}
                    interpoldict[cycle][value][length]=rbfi(cycle,value,length)
            pastvalue=value                         
            firsttime=False
   
       
        return deepcopy(interpoldict)

    def metadatadict(self,metafile=r'C:\Users\eivinhug\OneDrive - NTNU\PhD\Testing\Laminate_F\OBR_Files\MeasurementMetadata.txt'):

        # This list will be returned and contains just the positional data
        # and the values of the property of interest as tuples.
        data = []
        cycles = []
        In_File=metafile
        # Before the actual table with the data some metadata is written
        # into the files.
        # Right before the data a line with a keyword (e.g. "Length") appears.
        # This will sett data_is_here to True. As long as this is not the 
        # case, nothin will be written into the data-list defined above.
        data_is_here = False
        # Open the file with the data ...
        with open(In_File, 'r') as f:
            # ... scan through it line for line ...
            for line in f:
                # ... and if data_is_here is set to True ...
                if data_is_here:
                    # ... split this line at the "\t" (raw will be a list
                    # that contains the position as first value and the 
                    # value of the property of interest as second value but
                    # both will be strings and NOT numbers), ...
                    raw = line.strip().split('\t')
                    # ... convert the strings to numbers, ...
                  
                    a = int(raw[0])
                     
                    b = float(raw[1])
               
                    c = int(raw[2])
             
                    # ... create a tuple with these numbers ...
                    one_coord = (c, a, b)
                    # ... and append them to data; ...
                    try:
                        d=str(raw[3])
                    except:
                        d=''
                    if d=='Zero strain reference':
                        print("Measurement %s is zero strain reference at displacement %s" % (str(a),str(b)))
                        cycles.append(c)
                        data.append(one_coord) 
                    if "Use" in d:
                        print("%s instead of %s" % (str(d),a))
                        cycles.append(c)
                        a=int(d.split(" ")[1])
                        data.append((c, a, b))                
                    if d=="Skip":
                        print("Skipping measurement %s" % (str(a)))
                    if not d and b!=0.0:
                        cycles.append(c)
                        data.append(one_coord)
                # ... but if data_is_here is still False, check if one of the 
                # keywords appears, and if this is the case ...
                elif 'nr.' in line or 'dis' in line or 'Cycle' in line or 'type' in line or 'Comment' in line or 'Fiber number:' in line:
                    # ... set data_is_here to True.
                    data_is_here = True
        cycles=list(set(cycles))
        cyclessorted=sorted(cycles)

        metadict=defaultdict(dict)

        #print(data)
        for cycle in cyclessorted:
            for point in data:
                currentcycle=point[0]
                if cycle==currentcycle:
                    measurement=int(point[1])
                    displacement=float(point[2])
                    metadict[cycle][measurement]=displacement
                    
        #print(metadict)
        return metadict
        
    def metadatatuple(self,metafile=r'C:\Users\eivinhug\OneDrive - NTNU\PhD\Testing\Laminate_F\OBR_Files\MeasurementMetadata.txt'):

        # This list will be returned and contains just the positional data
        # and the values of the property of interest as tuples.
        data = []
        cycles = []
        In_File=metafile
        # Before the actual table with the data some metadata is written
        # into the files.
        # Right before the data a line with a keyword (e.g. "Length") appears.
        # This will sett data_is_here to True. As long as this is not the 
        # case, nothin will be written into the data-list defined above.
        data_is_here = False
        # Open the file with the data ...
        
        with open(In_File, 'r') as f:
            # ... scan through it line for line ...
            for line in f:
                # ... and if data_is_here is set to True ...
                if data_is_here:
                    # ... split this line at the "\t" (raw will be a list
                    # that contains the position as first value and the 
                    # value of the property of interest as second value but
                    # both will be strings and NOT numbers), ...
                    raw = line.strip().split('\t')
                    # ... convert the strings to numbers, ...
                  
                    a = int(raw[0]) #measurement nr.
                    
                    b = float(raw[1]) #load or displacement (value)
               
                    c = int(raw[2]) #Cycle
                    
                    d = str(raw[3]) #type (Skip or not)
                    
                    try:
                        e=int(raw[4]) #reference measurement
                    except:
                        if str(raw[4])=='first':
                            e='first'
                        else:
                            e=a-1
             
                    # ... create a tuple with these numbers ...
                    one_coord = (a, b, c, d, e)
                    # ... and append them to data; ...
                    data.append(one_coord)
                # ... but if data_is_here is still False, check if one of the 
                # keywords appears, and if this is the case ...
                elif 'nr.' in line or 'dis' in line or 'Cycle' in line or 'type' in line or 'Comment' in line or 'Fiber number:' in line:
                    # ... set data_is_here to True.
                    data_is_here = True

                    
        #print(metadict)
        return data


    def Measurement_Dictionary(self,File_Location='c:\Sweden\Bohuslan\ElectricAvenue',File_Name_Base='8a_trad'):

        onlyfiles = [f for f in listdir(File_Location) if isfile(join(File_Location, f))]
        allfilesdict={}
        for file in onlyfiles:
            if File_Name_Base in file and "Lower.txt" in file:
                d=float(file.split('_')[-2])
                onefiledict=smoothening().data_from_one_file(In_File=join(File_Location, file)) 
                allfilesdict[d] = onefiledict			
        return deepcopy(allfilesdict)

    def summedmeasurementfiles(self,Start_Of_Cycles=[],End_Of_Cycles=[],File_Location='',Name_Base=''):
        allsummedfilesdict={}

        for i in range(0,len(EndOfCycles)):	
            #Appends the summed strain into a dicitonary (note; '_Lower_summed.txt' in infile)
            maximum_number = int(EndOfCycles[i])
            minimum_number = int(StartOfCycles[i])

            for d in range(minimum_number,maximum_number+1):

                infile=os.path.join(file_location,name_base+(str(d)).zfill(4)+'_Lower_summed.txt')

                onefiledict=smoothening().data_from_one_file(In_File=infile)
                allsummedfilesdict[d] = onefiledict	

        return allsummedfilesdict
    
    def summedmeasurementdictionary(self,dictionary={},metadata=[]):

        data=deepcopy(dictionary)
        summedfile=defaultdict(dict)
        sumlist=[]
       
        
        measurements, values, cycles, comments, references =map(list, zip(*metadata))
        lengths=list(data[cycles[4]][values[4]].keys())
        pastcycle=10000000000000000000000000000000000000000
        
        for i in range(len(measurements)):
            
            cycle=cycles[i]
            value=values[i]
            measurement=measurements[i]
            comment=comments[i]
            reference=references[i]
            firsttime=True
            pastvalue=1000000000000000000
            
            if value!=pastvalue: #OK, so if cycle flips into a new ramp/new cycle, then 0 the summedstrain.
                firsttime=True            
            
            if cycle!=pastcycle: #OK, so if cycle flips into a new ramp/new cycle, then 0 the summedstrain.
                #firsttime=True
                summedstrain=np.array([0.]*len(list(data[cycles[3]][values[3]].values()))) # Random number to generate a list of zeros.            
            if comment!='Skip': #If the comment is not skip, the reading should be included
                if reference=='first': #If the first reference, the initiating reference, then the summedstrain array is set to the strain in the first measurement.
                    summedstrain=np.array(list(data[cycles[i]][values[i]].values()))
                    
                elif reference=='past': #If its a normal relative reference then; take the summedstrain list and add the data from the current measurement
                    summedstrain=summedstrain+np.array(list(data[cycles[i]][values[i]].values()))
                else: #If its a comment with a set reference measurement then:
                    integer=int([i for i,x in enumerate(measurements) if x == reference][0]) #find which integer that the reference holds in the metadatatuple.
                    summedstrain=np.array(list(summedfile[cycles[integer]][values[integer]].values()))+np.array(list(data[cycle][value].values())) #add up the summedstrain in the reference measurement with the strain in the current measurement.
            
            for t in range(len(summedstrain)):
                if firsttime: #if its the firsttime in the ramp, then initiate the dictionary at that cycle and that value
                    summedfile[cycle][value]={}
                    
                summedfile[cycle][value][lengths[t]]=summedstrain[t]
        
                firsttime=False
            pastcycle=cycle
            pastvalue=value
        return deepcopy(summedfile)

    def summedmeasurementdictionary2(self,dictionary={},metadict={},runningrefmeasurement=False,firstrunningrefmeasurement=2):

        sumdict=deepcopy(dictionary)
        sumindexdict={}
        firstsummeasurementlist=[]
        
        firsttime=True
        firstcycle=True
        
        for measurement,value in metadict[0].items():
            firstsummeasurementlist.append(measurement)
            if firstrunningrefmeasurement==measurement:
                break       
        
        for cycle, ramp in metadict.items():
            summeasurementlist=[]
            firsttime=True
            for measurement, value in ramp.items():
                if firsttime:
                    firstsummeasurementlist.append(measurement)
                    summeasurementlist=firstsummeasurementlist
                else: 
                    summeasurementlist.append(measurement) 
                    
                sumindexdict[measurement]=summeasurementlist
                firsttime=False
                
        for measurement, sumlist in sumindexdict.items():
            firsttime=True
            for measurmenttosum in sumlist:
                lengthlist,strainlist = zip(*list(dictionary[measurmenttosum].items()))
                if firsttime:
                    summedstrain=[0] * len(strainlist)
                summedstrain=summedstrain+strainlist
                firsttime=False
            for i in range(len(summedstrain)):
                sumdict[measurement][lengthlist[i]]=summedstrain[i]
                
        return sumdict


    def rbfidict(self,Dict={},lengthlist=[],cyclelist=[],valuelist=[],function='linear',smooth=0.0):
        interpolateddict=defaultdict(dict)
        for cycle, ramp in Dict.items():
            firsttime=True
            for value, reading in ramp.items():
                firsttime=True
                #print(measurement)
                mlist=[]
                llist=[]
                slist=[]
              
                for length, strain in reading.items():
    
                    mlist.append(value)
                    llist.append(length)
                    slist.append(strain)
                    x, y = np.array(llist),np.array(slist)
    
                rbfi=Rbf(x, y,function=function)    
                strainlist=rbfi(np.array(lengthlist))
                
                i=0
                for length in lengthlist:
                    strain=float(strainlist[i])
                    roundedstrain=round(strain,6)  
                    if firsttime:
                        interpolateddict[cycle][value]={} 
                    interpolateddict[cycle][value][length]=roundedstrain
                    i=i+1
                    firsttime=False
        
        return deepcopy(interpolateddict)
    
    def changemeasurementdict(self,measurementdict={},metadatalist=[]):
        newdict=defaultdict(dict)
        
        for entry in metadatalist:
            measurement=entry[0]
            cycle=entry[2]
            value=entry[1]
            comment=entry[3]            
            if comment!='Skip':
                if "Use" in comment:
                    newmeasurement=int(comment.split(" ")[1]) 
                    print("%s instead of %s" % (comment,str(measurement)))
                    measurement=newmeasurement      
                
                newdict[cycle][value]=deepcopy(measurementdict[measurement])
        
        return newdict
    
    
    def preparedictforplotting(self,lengthlist=[],measurements=[],dictionary={},plotvalue=1.8,newdict={}):
        MeasurementsAtPlotCycles=[]
        
        for length in lengthlist:
            
            if newdict!={}:
                strainatlength=[]
                for cycle in newdict.keys():
                    strainatlength.append(newdict[cycle][plotvalue][length])
            else:
                strainatlength=Difference_over_cycles3(measurements,length,dictionary)
                
            MeasurementsAtPlotCycles.append(strainatlength)

        return MeasurementsAtPlotCycles
    
    def numberandcyclesfromdisplist(self,disp=0.0,metadatadict={},findsmallestdispincycle=False):
        measurementatdisplist=[]
        cyclelist=[]
        for cycle, ramp in metadatadict.items():
            for measurement, displacement in ramp.items():
                if findsmallestdispincycle:
                    measurementlist,displist=zip(*list(ramp.items()))
                    disp=min(list(displist))    
                
                if displacement==disp:
                    measurementatdisplist.append(measurement)
                    cyclelist.append(cycle)

                    
        return measurementatdisplist,cyclelist
    
    def removenoise(self,noisedict={},data={}):
        dictionary=deepcopy(data)
        removednoisedict=defaultdict(dict)
        for cycle, ramp in dictionary.items():
            firsttime=True
            for value, straindata in ramp.items():
                firsttime=True
                for length, strain in straindata.items():
                    try:
                        noise=noisedict[cycle][value][length]
                    except:
                        if firsttime:
                            removednoisedict[cycle][value]={}
                        removednoisedict[cycle][value][length]=strain
                        firsttime=False
        return removednoisedict
    
    def areaundergraph(self,data={},lengthunit='m',strainunit='microstrain'):
        dictionary=deepcopy(data)
        intdict=defaultdict(dict)
        for cycle, ramp in dictionary.items():
            for value, measurement in ramp.items():
                firsttime=True
                for length, strain in measurement.items():
                    if firsttime:
                        dispsum=0.
                    else:
                        if lengthunit=='m':
                            dx=(length-pastlength)*1000
                        if lengthunit=='mm':
                            dx=(length-pastlength)                        
                        
                        if strainunit=='microstrain':
                            avgstrain=(strain+paststrain)/2000000
                        if strainunit=='strain':
                            avgstrain=(strain+paststrain)/2
                        dispsum=dispsum+(dx*avgstrain)
                    firsttime=False
                    pastlength=length
                    paststrain=strain
                intdict[cycle][value]=round(dispsum,2)
        
        return deepcopy(intdict)
    
    def plotsecondleveldict(self, data={}, plottype='3D'):
    
        if plottype == '3D':
            cycles=list(data.keys())
            intdata=defaultdict(dict)
            keylist=[]
            for cycle, ramp in data.items():
                for key, value in ramp.items():
                    keylist.append(key)
            keylist=list(set(keylist))
            keylist=sorted(keylist)
            
            for cycle, ramp in data.items():
                x=[]
                y=[]
                for key, value in ramp.items():
                    x.append(key)            
                    y.append(value)
                    
                rbfi=Rbf(x, y,function='linear')
                newy=rbfi(keylist)
                for i in range(len(keylist)):
                    key=keylist[i]
                    value=newy[i]
                    intdata[cycle][key]=value
                    
            Zlist=[]
            for key in keylist:
                keyatcyclelist=[]
                for cycle in cycles:
                    keyatcyclelist.append(intdata[cycle][key])
                Zlist.append(keyatcyclelist)
        
        
            fig = plt.figure(figsize=plt.figaspect(0.5))
            #fig.suptitle('Strain along fiber adjacent to hole at %s mm displacement (max displacement was 3.6 mm)'%(str(plotdisp)), fontsize=16)
        
            X1,Y1=np.meshgrid(cycles,keylist)
            Z1=np.array(Zlist)
            ax = fig.add_subplot(1, 1, 1, projection='3d')
            #ax.set_zlim(0, 12000)
            #ax.plot_wireframe(X1, Y1, Z1, rstride=1, cstride=1)
            ax.set_xlabel('Cycles')#, fontsize=20)
            ax.set_ylabel('Displacement')	
            ax.set_zlabel('Force')	
            surf = ax.plot_surface(X1, Z1, Y1,cmap=cm.coolwarm,
                                       linewidth=0, antialiased=False)#,cmap=cm.coolwarm,linewidth=0, antialiased=False)	
        
            fig.colorbar(surf, shrink=0.5, aspect=5)
        
            plt.show()
        if plottype == '2D':
            cycles=list(data.keys())
            for cycle, ramp in data.items():
                keylist=[]
                valuelist=[]
                for key, value in ramp.items():
                    keylist.append(key)
                    valuelist.append(value)
                    
                plt.plot(valuelist,keylist,label=str(cycle)) 
                plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
            plt.show()
    
    
