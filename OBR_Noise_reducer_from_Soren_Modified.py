#    "OBR_Noise_reducer" (v1.0)
#    Copyright 2016 Soren Heinze
#    soerenheinze <at> gmx <dot> de
#    5B1C 1897 560A EF50 F1EB 2579 2297 FAE4 D9B5 2A35
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.

'''
This file has been altered to fit the needs of Phd Candidate Eivind Hugaas
In general, all file administration has been taken out and only the smoothening algorithm left in place.
It now takes in only list of tuples and gives out list of tuples, along with the PNG files.
Some errors were found with the general window, theese have been changed so that now it smoothens somehow more aggressively.

'''

# A small program to automatically detect outliers in analyzed OBR data.
# 
# This program was written for files generated by the OBR-measurement/analyzing
# program, as stated in the accompanying manual to this program, in connection
# with the OBR_analyzer_simple.py program.
# 
# It is assumed that just ONE property if interest (e.g. either strain OR 
# temperature) is saved in each file.
# In this case the left column should contain the positional values and 
# the right column the values for the property of interest.
# 
# Before the table with the actual values some other data is present in the 
# analysis-file.
# However the table starts usually with some keywords directly followed
# by the data.
# In the functions below I use the following keywords: "Length", "microstrain", 
# "(deg C)", "Spectral". This should cover the most common cases. If this is not
# the case, extra keywords can simply be added in these functions.
# 
# It is also assumed that neither the central position of the area of interest, 
# or other (virtual) strain gauge parameters changed during the analysis of 
# the OBR data.
# In general (and especially ) this should be the case.
# 
# In addition the following assumptions are made regarding the data
# 0. the data is mainly "good" and has some outliers
# 1. the data is sectionwise "flat"
# 2. three (or more) "flat points" are considered to be one section
# 3. linear relation between flat sections
# 4. equally spaced points in x-direction
# 5. I actually need TWO flat sections to be able to "interpolate"
# 
# Due to rounding errors of the OBR analysis program, assumption 4 is not 
# entirely true. But the error in position is far below the measurement
# resolution and this program compensates for such eventualities.
# 
# Since some outliers escape this algorithm, this program creates (initially) 
# identical noise reduced files in two locations. So that the user can 
# manually correct the remaining outliers.
# See also the accompanying manual to this program.
# 
# For higher user friendlyness to detect the remaining outliers, each outlier 
# corrected dataset is automatically plotted and saved as a PNG file.
# This requires the installation of matplotlib.
# See the relevant document regarding this issue.
# 
# ATTENTION: It is assumed that the OBR_Filesorter.py program was used to sort 
# the analysis files. This includes the creation of certain folders that are 
# assumed to exist throughout this program. 
# If not these folders need to be created. 
# See also the accompanying manual to this program.
# 
# The parameters: maximum_difference_between_points (to determine how flat a 
# "flat section" is (see also point 1 above) and 
# maximum_difference_for_interpolation (as maximum allowed deviation from the 
# linear line between two flat sections to still count as datapoint, see also
# point 3 above) worked out fine for me, but probably need to be adjusted.
# 
# The main part of this file is the definition of functions. The actual 
# execution of these functions (a.k.a. running the program) takes place at the 
# end of this file.
# 
# ATTENTION: It is assumed that the running reference approach was used to 
# analyze the OBR rawdata.
# This means that from measurement two onwards, each file was analyzed
# once with the last measurement as reference and once with the second to last
# measurement as reference file. The filenames of the latter analysis should 
# contain "_older_reference" to indicate this.
# This is the case of the OBR_analyzer_simple.py program was used for the 
# analysis of the OBR rawdata.

# On older computers it may take a while before all modules are  loaded.
# Impatient users may think the program crashed. Hence, this message.

# print "Loading modules (this may take some seconds) ..."

from copy import deepcopy
import matplotlib.pyplot as plt
import os
# shutil is here for using the correct operative specific move commands.
import shutil
from math import log
import numpy as np

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d.axes3d import Axes3D, get_test_data
from matplotlib import cm
import numpy as np

# This function reads the data from a file and returns a list that
# contains tuples which contain the data in the correct order.
# The correct order is important to find the "flat" sections.
def data_from_one_file(infile):
	# This list will be returned and contains just the positional data
	# and the values of the property of interest as tuples.
	data = []
	# Before the actual table with the data some metadata is written
	# into the files.
	# Right before the data a line with a keyword (e.g. "Length") appears.
	# This will sett data_is_here to True. As long as this is not the 
	# case, nothin will be written into the data-list defined above.
	data_is_here = False
	# Open the file with the data ...
	with open(infile, 'r') as f:
		# ... scan through it line for line ...
		for line in f:
			# ... and if data_is_here is set to True ...
			if data_is_here:
				# ... split this line at the "\t" (raw will be a list
				# that contains the position as first value and the 
				# value of the property of interest as second value but
				# both will be strings and NOT numbers), ...
				raw = line.strip().split('\t')
				# ... convert the strings to numbers, ...
				a = float(raw[0])
				b = float(raw[1])
				# ... create a tuple with these numbers ...
				one_coord = (a, b)
				# ... and append them to data; ...
				data.append(one_coord)
			# ... but if data_is_here is still False, check if one of the 
			# keywords appears, and if this is the case ...
			elif 'Length' in line or 'microstrain' in line \
						or '(deg C)' in line or 'Spectral' in line:
				# ... set data_is_here to True.
				data_is_here = True
	
	#Make Data a Dict, such as all other data:
	DataDict={}
	for point in data:
		transfer_point_into_dict(DataDict, point)

	# When the end of the file is reached, return the data.
	#print(data)
	return DataDict



# This function reads the data from the previous measurement and the 
# measurement with an older reference. It returns a dict (!).
# I don't need the correct order later, but the ability to access
# the y-values to given x-values directly.
# 
# This is very much like data_from_one_file() with some exceptions
# I mention below.
def get_extra_data(other_reference_measurement, previous_measurement):
	# In contrast to data_from_one_file() I create here dictionaries and
	# NOT lists.
	other_reference = {}
	previous = {}

	data_is_here = False
	# First, get the data from the file with the second to last measurement
	# as reference.
	with open(other_reference_measurement, 'r') as f:
		for line in f:
			if data_is_here:
				raw = line.strip().split('\t')
				a = float(raw[0])
				b = float(raw[1])
				# Here is another diference to data_from_one_file(). Since
				# 
				other_reference[a] = b
			elif 'Length' in line or 'microstrain' in line \
						or '(deg C)' in line or 'Spectral' in line:
				data_is_here = True

	# Don't forget to set data_is_here back to False because otherwise all the 
	# metadata will be read and that will lead to errors down the line.
	data_is_here = False

	# Second, get the data from the previous measurement.
	with open(previous_measurement, 'r') as f:
		for line in f:
			if data_is_here:
				raw = line.strip().split('\t')
				a = float(raw[0])
				b = float(raw[1])
				previous[a] = b
			elif 'Length' in line or 'microstrain' in line \
						or '(deg C)' in line or 'Spectral' in line:
				data_is_here = True

	return (other_reference, previous)



# This function can checks if the two points to the left/right to points 
# 
# data is the list that contains the datapoints as tuples.
# which_point is the position of the datapoint in question in this list;
# NOT the "x"-value!
# ATTENTION: which_point must be >= 2!
# ATTENTION 2: remember that counting starts at ZERO!
# ATTENTION 3: The first two points are NOT to be evaluated.
# See also find_flat_sections().d
# I could consider those points too, but that would require some
# if-statements I deem not necessary.
def check_to_the_side(data, which_point, maximum_difference_between_points, \
												which_direction = 'right'):
	# one_step is used to check one (or two) step(s) to the left/right.
	# In case I need to check to the right one_step needs to be positive ...
	one_step = 1
	# ... in case I need to check to the left, one_step needs to be negative.
	if which_direction == 'left':
		one_step = -1

	# Don't forget, that data is a list, that contains tuples.
	value = data[which_point][1]
	first_side_value = data[(which_point + one_step)][1]
	second_side_value = data[(which_point + 2*one_step)][1]

	# The values have to be in "one line" and the difference
	# between them shall not be greater than a given value.
	difference_point_to_first = abs(value - first_side_value)
	difference_first_to_second = abs(first_side_value - second_side_value)

	# Just if all three points are in one line (with the given allowed
	# difference between two points), this point is considered to be
	# in a straight section.
	if difference_point_to_first <= maximum_difference_between_points and \
			difference_first_to_second <= maximum_difference_between_points:
		return True
	else:
		return False



# This is more or less the same as check_to_the_side() but here I check the 
# point to left and the point to the right.
def check_middle(data, which_point, maximum_difference_between_points):
	value = data[which_point][1]
	left_side_value = data[(which_point - 1)][1]
	right_side_value = data[(which_point + 1)][1]

	difference_to_left = abs(value - left_side_value)
	difference_to_right = abs(value - right_side_value)

	if difference_to_left <= maximum_difference_between_points and \
			difference_to_right <= maximum_difference_between_points:
		return True
	else:
		return False



# This is just a summary of the check_to_the_side() and check_middle()
# to have just one line in later sections of the program.
def belongs_to_straight_section(data, which_point, maximum_difference_between_points):
	if not check_to_the_side(data, which_point, maximum_difference_between_points, \
				'right') and not check_to_the_side(data, which_point, \
				maximum_difference_between_points, 'left') and not \
				check_middle(data, which_point, maximum_difference_between_points):
		return False
	else:
		return True



# I need to transfer quite often data into dicts throughout this program.
# Thus I created just function for that to keep the functions where this 
# is done more orderly.
# point must be the tupel (x_value, y_value).
def transfer_point_into_dict(the_dict, point):
	x_value = point[0]
	y_value = point[1]
	the_dict[x_value] = y_value
	
#Due to standardazing of format to dict, some of the older functions has to have the following function.	

def transform_dict_to_list(dictionary):
	dictlist=[]
	for key, value in dictionary.items():
		temp = [key,value]
		dictlist.append(temp)		
		
	return dictlist


# This functions figures out if which of the datapoints in data are within a 
# flat section (three points in one line, without outliers in between).
# 
# This function combines belongs_to_straight_section() and 
# transfer_point_into_dict() to actually find the flat sections in the data 
# and transfer it to the "these points are ok"-data.
def find_flat_sections(data, maximum_difference_between_points):
	# Create the containers for the real data and the outliers.
	# After this function is finished, real_data will contain so far just
	# the points in flat sections.
	# outliers contains all the remaining points. 
	# If outliers turn out to be real data at a later step, these will be 
	# transfered into real_data and deleted from outliers.
	real_data = {}
	outliers = {}
	# For EACH of the datapoints it will be checked if it belongs to
	# a flat section. If yes, it will be transfered into real_data, 
	# if not, into outliers.
	#
	# ATTENTION: DON'T EVALUATE THE FIRST TWO AND THE LAST TWO POINTS!
	# THE FUNCTIONS ABOVE CAN'T DO THAT!
	# With the data I have, this doesn't matter, since I usually have 
	# a lot of unnecessary data at the sides.
	if type(data) is dict:
		data=transform_dict_to_list(data)
	
	for i in range(2, (len(data) - 2)):
		# If a point belongs to a flat section ...
		if belongs_to_straight_section(data, i, maximum_difference_between_points):
			# ... transfer it to real_data ...
			transfer_point_into_dict(real_data, data[i])
		# ... otherwise, ...
		else:
			# ... transfer it ti outliers.
			transfer_point_into_dict(outliers, data[i])

	return (real_data, outliers)

# Added by EH to take care of extreme increases in strain:
def max_strain_increase(data, maxallowedstrainincrease):
	
	if type(data) is dict:
		data=transform_dict_to_list(data)
	realdata=[]
	unrealdata=[]
	realdatadict={}
	unrealdatadict={}
	
	for i in range(len(data)):
		if abs(data[i][1])<maxallowedstrainincrease:
			realdata.append(data[i])
		else:
			unrealdata.append(data[i])
	for point in realdata:
		transfer_point_into_dict(realdatadict,point)	
		
	for point in unrealdata:
		transfer_point_into_dict(unrealdatadict,point)		
	
	return realdatadict, unrealdatadict

# To calculate the distance between points in x-direction.
# I need this quite often so I wrote an extra function for that.
# real_data is a dict.
# ATTENTION: This function assumes that the points are equally spaced 
# in x-direction. Hence it just 
def calculate_constant_x_distance(real_data):
	x_values = sorted(real_data.keys())
	# Not all points are equally spaced in x-direction. See also the comment 
	# above and in find_start_and_end_of_unknown_sections().
	#  So I figure first out all possible distances between two adjecent points
	#  and put them into a set and into a list.
	# A set contains just unique values. So if a value shall be added to 
	# the set that already is in the set, it will not be added again.
	# However, in a list two identical values can be in there.
	# Afterwards, I check for each element in the set how often it appears
	# in the list and return the most common value.	
	set_of_all_distances = set()
	all_distances = []

	# First figure out all distances between the points and add them 
	# to the list and set ...
	for i in range(len(x_values) - 1):
		this_distance = x_values[i + 1] - x_values[i]
		set_of_all_distances.add(this_distance)
		all_distances.append(this_distance)

	# ... then find the distance that is most common.
	so_often = 0
	constant_x_distance = None
	for distance in set_of_all_distances:
		so_often_new = all_distances.count(distance)
		if so_often_new > so_often:
			so_often = so_often_new
			constant_x_distance = distance
	print(constant_x_distance)
	return constant_x_distance



# This function checks the distance between all the points in the 
# flat sections. If the distance between two points is bigger then the most 
# common distance, these two points are the start- and end-point of the
# unknown section.
# See also the comment below.
def find_start_and_end_of_unknown_sections(real_data):
	start_end_points = []
	# It is assumed that there actually are at least two flat sections.
	if len(real_data) == 0:
		# If this is not the case, the data is just noise.
		print("The data is just noise")
	elif len(real_data) == 1:
		# If this is not the case, the data is just noise.
		print("The data is just noise")	
	else:
		x_values = sorted(real_data.keys())
		# All points have (in theory) the same (characteristic) distance from 
		# each other in x-direction (position or length). That means that a gap
		# between two flat sections due to an outlier is equivalent
		# to two points NOT having this distance (in x-direction) to each 
		# other. Since I look just at the already determined flat section
		# data here, I should easily figure out these gaps by checking if 
		# two points have a different distance from each other than this
		# characteristic distance.
		# However, as mentioned above sometimes the positional value is 
		# NOT equidistant to the previous or following point due to rounding 
		# errors of the OBR measurement/analysis program. The error is very 
		# small and may be positive or negative. 
		# So simply checking if the distance between two points is not 
		# the characteristic distance may "find" gaps where there actually are
		# none.
		# BUT a gap contains at minimum width one datapoint. So two separate flat
		# sections are (in theory) at least two characteristic distances apart. 
		# That means checking if two points are more then 1 1/2 characteristic 
		# distances apart from each other will detect just real gaps between 
		# the flat sections.
		# 
		# This is an issue that comes up at several places throughout 
		# this whole program.
		minimum_distance_for_gap = calculate_constant_x_distance(real_data)
		minimum_distance_for_gap += minimum_distance_for_gap / 2

		# The last flat section can NOT have a "continuation" to the
		# "next" flat section (since it already is the last).
		# Hence it doesn't matter that I don't check the very last value.
		for i in range((len(x_values) - 1)):
			x_distance = abs(x_values[i] - x_values[i + 1])
			# Just if the distance between two points is significantly bigger 
			# then the most common distance between points it shall be 
			# recognized as a gap.
			if x_distance > minimum_distance_for_gap:
				start = (x_values[i], real_data[x_values[i]])
				end = (x_values[i + 1], real_data[x_values[i + 1]])
				start_end_points.append((start, end))

	return start_end_points



# This is to find the slope between two given points.
# intervalls is a list that contains tuples. Each tuple containins two tuples, 
# that contain the x- and y-coordinates.
# See find_start_and_end_of_unknown_sections()
def find_slopes_of_undefined_sections(intervalls):
	all_section_data = []
	# If there are no gaps in the data, I can not interpolate.
	# Most often this is because the data is just very good and does not 
	# have outliers.
	if len(intervalls) == 0:
		print ("No intervalls to interpolate between. No worries, probably it is just good data :) .")
	else:
		# For each gap ...
		for intervall in intervalls:
			# ... simply calculate the linear slope between two points.
			y_difference = intervall[1][1] - intervall[0][1]
			x_difference = intervall[1][0] - intervall[0][0]
			slope = y_difference / x_difference

			# In the end add the slope value to the start-/end-point tuples.
			# Since one can not append or add to tuples, I need to create
			# whole new tuples.
			all_section_data.append((intervall[0], intervall[1], slope))
	
	return all_section_data



# This is just a summary of the find_start_and_end_of_unknown_sections() and
# find_slopes_of_undefined_sections() to have just one line in reduce_noise().
# Since this will run after the initial check for "flat"-sections it takes
# this data as input.
def find_undefined_sections(first_run_real_data):
	# First find the start- and end-point of the gaps between the flat 
	# sections ...
	undefined_sections = find_start_and_end_of_unknown_sections(first_run_real_data)
	# ... then calculate the slope of the line between these two points.
	undefined_sections_with_slopes = find_slopes_of_undefined_sections(undefined_sections)

	return undefined_sections_with_slopes



# To know how many points there are in the intervall between two flat sections.
# startpoint and endpoint are tuples.
def so_many_points_in_intervall(startpoint, endpoint, constant_x_distance):
	# I actually calculate NOT the number of points but how many "jumps"
	# of constant_x_distance are there between startpoint and endpoint.
	# The number of "jumps" must of course be reduced by one to get the
	# number of points.
	# 
	# Since int() returns everything before the comma, this is 
	# unaffected by the above mentioned rounding errors in the x-values
	# of the points.
	return int((endpoint[0] - startpoint[0])/constant_x_distance - 1)



# To calculate where the point should be if there is a linear 
# slope between two flat sections.
def calculate_point(start, constant_x_distance, slope, which_point):
	point_x = start[0] + constant_x_distance*(which_point + 1)
	# This is how the y-value should be.
	point_y = slope*(constant_x_distance*(which_point + 1)) + start[1]

	return (point_x, point_y)



# Calculations sometimes come up with values that are not exactly as
# measured, in this case the nearest value is most likely the one I'm
# looking for and shall be used.
# find_nearest() searches the whole dict for the value I'm looking for.
def find_nearest(dictionary, x_value_to_be_found):
	x_values = dictionary.keys()
	old_difference = 10000000000000000000000.0
	nearest_value = -50000000000000000000000.0

	# The keys in the dicts I use in this program are the x-values of the 
	# points.
	# This loop calculates for all points the difference to the x-value
	# I'm looking for but "remembers" just the smallest difference and the
	# x-value it belongs to. This will be returned once all values are checked.
	for x_value in x_values:
		difference = abs(x_value - x_value_to_be_found)
		if difference < old_difference:
			old_difference = difference
			nearest_value = x_value

	return nearest_value



# This compares the calculated y-value with the measured y-value
# and returns True if the difference is <= maximum_difference_for_interpolation.
# 
# Since the many times mentioned rounding errors of the x-values, it also 
# returns the x_value that shall be used when saving this point.
def point_is_real(outliers, calculated_point, maximum_difference_for_interpolation):
	# calculated_point contains the "perfect" x-values if all points have
	# absolutely the same distance.
	# Again due to the rounding errors, this may not be the real x-value
	# and thus will not be found in the outliers-dict. If this is the case ...
	try:
		x_value = calculated_point[0]
		real_point_y = outliers[x_value]
	# ... find the point that is nearest to this "perfect" calculated position.
	# I can do it this way, since the rounding errors are small and the actual
	# point is always the nearest to this calculated "perfect" position.
	except KeyError:
		x_value = find_nearest(outliers, calculated_point[0])
		real_point_y = outliers[x_value]

	# Calculate the difference between tha actual point and the expected
	# value if this point would have been on the line between flat sections.
	difference = abs(real_point_y - calculated_point[1])

	# If this difference is small enough ...
	if difference <= maximum_difference_for_interpolation:
		# ... return True and what the (actual) x-value of this point is.
		return (True, x_value)
	else:
		return (False, None)



# This function returns JUST the x-values of the points that turn out
# to be NOT outliers!
def find_more_points_in_one_intervall(intervall_data, constant_x_distance, outliers, \
													maximum_difference_for_interpolation):
	# A list that contains all x-values of the points that are figured out as 
	# real values.
	real_points = []
	start = intervall_data[0]
	end = intervall_data[1]
	slope = intervall_data[2]

	# Calculate how many point there actually need to be in an intervall.
	points_in_intervall = so_many_points_in_intervall(start, end, constant_x_distance)
	for i in range(points_in_intervall):
		# Then for each of these assumed point calculate the x-values they 
		# would have if the point would be exactly on the line.
		assumed_point = calculate_point(start, constant_x_distance, slope, i)
		# Afterwards check if the actual point is in the vicinity of this
		# calculated point.
		# real_point is a tuple that contains True if the point is real and
		# the x-value of this point.
		real_point = point_is_real(outliers, assumed_point, \
											maximum_difference_for_interpolation)
		# If the point in question is NOT an outlier ...
		if real_point[0]:
			# ... append its x-value to the list ...
			real_points.append(real_point[1])

	# ... and in the end return the list with all the x-values of the points 
	# that turned out not to be outliers.
	return real_points



# To check if real data points were dismissed initially because these
# were not in a "flat"-section.
# See also comment in reduce_noise().
# Returned are the (probably expanded) real data dict and the 
# (probably reduced) outlier dict.
def check_if_point_is_on_line(undefined_sections, first_run_real_data, \
					first_run_outliers, maximum_difference_for_interpolation):
	# This is probably not necessary but I don't like to mess with results
	# I already have, so I prefer to make a real copy.
	# more_real_data will be first_run_real_data with more points added to 
	# it; less_outliers will be first_run_outliers with points taken out of it.
	# Hence the names.
	more_real_data = deepcopy(first_run_real_data)
	less_outliers = deepcopy(first_run_outliers)

	# If there were no flat sections, the data is just noise.
	if len(more_real_data) == 0:
		print ("The data is just noise")
	else:
		# This is just to figure out (again) the distance in x-direction.
		constant_x_distance = calculate_constant_x_distance(more_real_data)
		# Now the real thing starts.
		for intervall in undefined_sections:
			# find_more_points_in_one_intervall() figures out points in the 
			# intervall (gap) that actually are NOT outliers and returns
			# the x-values of these points.
			# more_points are just the x-values!
			more_points = find_more_points_in_one_intervall(intervall, \
									constant_x_distance, less_outliers, \
									maximum_difference_for_interpolation)

			# Then find the y-values to these x-values ...
			for x_value in more_points:
				point = (x_value, less_outliers[x_value])
				# ... add the (now as real recognized) points to the
				# real data dict ... 
				transfer_point_into_dict(more_real_data, point)
				# ... and at the end delete the point from the outliers.
				del less_outliers[x_value]

	return (more_real_data, less_outliers)



# Sometimes outliers actually are peaks that show up consistently.
# If this is the case a check of the same data, obtained in a different way
# may reveal these peaks.
# This function does that.
# 
# other_reference_data is the same measurement but evaluated with a 
# reference two measurements back (and not just one)
# previous_data is the measurement from before evaluated in the regular way.
# If the supposed outlier is real it's value, added to previous_data should 
# add upp to other_reference_data.
# 
# ATTENTION: It is possible that noise from all the data compared in this 
# function adds up so that an outlier is actually seen as real data.
# In a testcase this happened approx. 5 times in 50.000 datapoints.
# But these points are "classical" outliers and will be detected in the 
# very last step.
def check_other_data_for_peaks(second_run_real_data, second_run_outliers, \
			other_reference_data, previous_data, maximum_difference_between_points):
	more_real_data = deepcopy(second_run_real_data)
	less_outliers = deepcopy(second_run_outliers)
	#print(less_outliers.keys())
	# .keys() returns just the x-values of the points.
	for outlier_x in list(less_outliers):# less_outliers.keys():
		# The same issue that appears due to rounding errors. 
		# See also e.g. comment in point_is_real() since there the same 
		# try ... except is implemented due to the same reason.
		try:
			calculated = less_outliers[outlier_x] + previous_data[outlier_x]
		except KeyError:
			# I don't need to "correct" for the values in less_outliers since
			# I take the value from there. 
			correct_x_value = find_nearest(previous_data, outlier_x)
			calculated = less_outliers[outlier_x] + previous_data[correct_x_value]

		try:
			difference = abs(calculated - other_reference_data[outlier_x])
		except KeyError:
			correct_x_value = find_nearest(other_reference_data, outlier_x)
			difference = abs(calculated - other_reference_data[correct_x_value])

		if difference <= maximum_difference_between_points:
			real_point = (outlier_x, less_outliers[outlier_x])
			transfer_point_into_dict(more_real_data, real_point)
			del less_outliers[outlier_x]

	return (more_real_data, less_outliers)



# For adding up the data I need to fill in values. 
# Here it is assumed that all remaining outliers actually are outliers, thus 
# I don't delete any longer from the outlier dict.
def interpolate_values(undefined_sections, last_run_real_data):
	even_more_real_data = deepcopy(last_run_real_data)
	constant_x_distance = calculate_constant_x_distance(even_more_real_data)

	for intervall_data in undefined_sections:
		
		start = intervall_data[0]
		end = intervall_data[1]
		slope = intervall_data[2]

		# Calculate how many point there actually need to be in an intervall.
		points_in_intervall = so_many_points_in_intervall(start, end, \
															constant_x_distance)
		for i in range(points_in_intervall):
			# Then calculate for each of these points the value it should have
			# on the slope.
			assumed_point = calculate_point(start, constant_x_distance, slope, i)

			# Find the nearest point to that calculated point.
			nearest_point = find_nearest(even_more_real_data, assumed_point[0])
			difference = abs(assumed_point[0] - nearest_point)
			# This method interpolates all points between two flat sections.
			# But in the steps before I already added points in such gaps
			# if these turned out not to be outliers.
			# 
			# As mentioned already several times, do rounding errors occur
			# in the x-values. So the constant_x_distance is not as constant 
			# in real data as I assumed. 
			# 
			# Hence if I just search for assumed_point[0] to be in 
			# even_more_real_data I may add points which are already in there 
			# but which are a bit longer/lesser away from the last point than 
			# constant_x_distance. 
			# 
			# However, IF the point is already in even_more_real_data (so if it 
			# was recognized as a real point in a previous step), its x-value
			# is very near assumed_point[0] since these rounding errors are 
			# small. It is in any case nearer then constant_x_distance/2.
			# 
			# Thus if the nearest point to the assumed_point is farther away 
			# then this distance, assumed_point must be added to the real
			# data since there is no point at this position, yet.
			if difference >= constant_x_distance/2:
				transfer_point_into_dict(even_more_real_data, assumed_point)

	return even_more_real_data



# There can be data before the first or after the last flat section. 
# This may be noise, or real data with a steep slope (thus not "flat"). 
# I can NOT interpolate in this case. Thus I just fill up with the 
# original value.
# original_data is a list, last_run_real_data is a dict.
def fill_up_the_data(original_data, last_run_real_data):
	
	if type(original_data) is dict:
		original_data=transform_dict_to_list(original_data)
	
	final_real_data = deepcopy(last_run_real_data)
	constant_x_distance = calculate_constant_x_distance(last_run_real_data)

	# Yes, here I go through the complete original data.
	for x, y in original_data:
		if x not in final_real_data:
			nearest_x = find_nearest(last_run_real_data, x)
			# See comment in interpolate_values why I use 
			# constant_x_distance/2 here.
			if abs(nearest_x - x) >= constant_x_distance/2:
				final_real_data[x] = 10000 #y

	return final_real_data



# To write the noise reduced data into a file.
# Since here the order is important again, final_data needs to be a list, 
# that contains the tuples with coordinates.
def write_to_file(outfile, final_data):
	with open(outfile, 'w') as f:
		for x, y in final_data:
			f.write('%s\t%s\n' % (x, y))



# This function makes pictures of each data-set that shall help me with 
# manually figuring out the last remaining outliers that are not detected
# by this program.
def plot_data(original_data, reduced_noise_data, save_file):
	with_noise = []
	without_noise = []
	
	if type(original_data) is dict:
		original_data=transform_dict_to_list(original_data)	
	
	for x, y in original_data:
		with_noise.append(y)
	for x, y in reduced_noise_data:
		without_noise.append(y)

	# If pyplt gets just a list, it fills up the x-values automatically.
	# I don't really care about the absolute x-position here, since I just 
	# need to compare the y-values at the same position.
	# just need a comparison so I 
	plt.plot(with_noise, 'ro', without_noise, 'bs')
	plt.plot(with_noise, 'ro')
	plt.plot(without_noise, 'bs')
	#plt.plot(with_noise, 'ro', without_noise)
	#plt.plot(with_noise, 'ro')
	#plt.plot(without_noise, 'bs')	
	plt.title('RED = with noise, BLUE = withOUT noise')
	# Save the plot
	
	
	plt.savefig(save_file)
	# Clear the plot
	plt.clf()


# One function that does the stuff above.
# ATTENTION: THIS WORKS JUST FOR DATA THAT HAS AN AVAILABLE
# PREVIOUS MEASUREMENT AND A FILE WITH ANOTHER REFERENCE!
# 
# Hence this can NOT be used for the very first measurement!!!

def plot_tuple(data):
	testList = data
	testList2 = [(elem1, elem2) for elem1, elem2 in testList]
	zip(*testList2)
	plt.scatter(*zip(*testList2))
	plt.show()		

def SmoothenOverLength(allfilesdict,MeasurementsToSmoothen,allfilesMDiffDictonlyNoise):
	NotFinished='yes'



def reduce_noise(infile, outfile, binarynoisefile, other_reference_measurement, previous_measurement, plotfile, maximum_difference_between_points, maximum_difference_for_interpolation, maxallowedstrainincrease):
	# Say that nr. 5 is the measurement file in question
	
	# First read the data from the actual analysis file to be checked for 
	# outliers.
	# This will be the file with the last measurement as reference.

	data = data_from_one_file(infile) #E01_fibre 1_0005_Lower.txt input, returns Dict.
	
	#plot_tuple(data)
	
	# Second get the data from the same measurement but with the second to last 
	# measurement as reference and also the data from the previous analysis.
	# Both are needed to check if an outlier probably is real data and just looks
	# like an outlier if just the data for this one analysis is taken into account.
	
	extra_data = get_extra_data(other_reference_measurement, previous_measurement) #other ref is the second to last ref as ref, this can be fixed ffs. previous measurement is E01_fibre 1_0004_Lower.txt

	other_reference_measurement_data = extra_data[0]
	previous_measurement_data = extra_data[1]

	# Second added (EH):
	
	ThresholdCheck=max_strain_increase(data, maxallowedstrainincrease)

	Thresholded_data=ThresholdCheck[0]
	Threshold_data_outliers = ThresholdCheck[1]
	
	# Third, find the flat sections in this data ...
	first_check = find_flat_sections(Thresholded_data, maximum_difference_between_points)
	# ... and sort into as real assumed data (the points in the flat sections) ...
	
	first_run_real_data = first_check[0] #Dict
	
	# ... and possible outliers (all points NOT in flat sections).
	
	first_run_outliers = first_check[1] #Dict
	
	first_run_outliers.update(Threshold_data_outliers)
	
	# Fourth, figure out the start- and end-points of the non-flat sections
	# and what the slope of the line between these points is.
	undefined_sections = find_undefined_sections(first_run_real_data)
	
	# Fifth, check if points that were assumed to be outliers, because these were
	# not in a flat section, probably are on the line between the start- and
	# end-point between two flat sections (vulgo: in the gap).
	# The points can "jitter" a bit around the line in y-direction.
	# How big this "jittering" can be is determined by 
	# maximum_difference_for_interpolation.
	# 
	# I assume that "dynamic regions" (e.g steep strain gradients) probably 
	# also lead to higher "jitter" values without the value of the property of 
	# interest being an outlier. To take this into account, 
	# maximum_difference_for_interpolation is bigger then 
	# maximum_difference_between_points which determines how "flat" a flat
	# section needs to be.
	# 
	# So this check is more or less just checking if the y-value of a point
	# is not larger or smaller then point then a the value determined by the
	# expected line in the gap.
	# 
	# Formerly assumed outliers that turn out to be real data during this 
	# check are transferd from the outlier-dict to the real-data-dict.
	# and both (modified) dicts are returned by check_if_point_is_on_line() 
	# and become second_run_real_data and second_run_outliers below.
	
	second_check = check_if_point_is_on_line(undefined_sections, first_run_real_data, first_run_outliers, maximum_difference_for_interpolation)
	second_run_real_data = second_check[0]
	second_run_outliers = second_check[1]

	# Sixth, if outliers show up at the same position and with approx. the same
	# value in the measurement with the older reference, it is very likely that 
	# these are actually NOT outliers but real values. Here this will be 
	# checked to reduce the chance of falsely dismissing real data.
	# See also comment to check_other_data_for_peaks().
	# The dicts with the real data and the outliers will be changed accordingly
	# and returned by check_other_data_for_peaks().
	
	# EH added. But before that, go through the other data and check that its reasonable:
	
	ThresholdCheckOther=max_strain_increase(other_reference_measurement_data, maxallowedstrainincrease)
	
	Thresholded_other_reference_measurement_data=ThresholdCheckOther[0]
	
	ThresholdCheckPrevious=max_strain_increase(previous_measurement_data, maxallowedstrainincrease)
	
	Thresholded_previous_measurement_data=ThresholdCheckPrevious[0]
	
	third_check = check_other_data_for_peaks(second_run_real_data, \
			second_run_outliers, Thresholded_other_reference_measurement_data, \
			Thresholded_previous_measurement_data, maximum_difference_between_points)
	third_run_real_data = third_check[0]
	third_run_outliers = third_check[1]
	
	# Seventh, when using the running reference method to obtain the strain 
	# data, I need to add up all the different measurements. This means that
	# if outliers are detected, these can not just be left out, but must be
	# substituted in a meaningful way. The substituted values will be all
	# on the the line between two flat sections.
	
	undefined_sections = find_undefined_sections(third_run_real_data)
	
	fourth_run_real_data = interpolate_values(undefined_sections, third_run_real_data)

	# Eigth, situations may occur in which I can not interpolate. In these cases
	# just fill in the original data.
	# See also comment to fill_up_the_data()
	
	final_real_data = fill_up_the_data(data, fourth_run_real_data)
	
	final_real_data=fourth_run_real_data
	
	# Ninth, since a dict is unsorted, I need to sort the data before writing to
	# file.
	sorted_x_values = sorted(final_real_data.keys())
	data_to_be_written = []
	
	for x in sorted_x_values:
		data_to_be_written.append((x, final_real_data[x]))

	# Tenth, write the sorted list to the outfile.
	write_to_file(outfile, data_to_be_written)

	# Eleventh, due to "lucky" add-up of noise data some noise-points are 
	# considered to be real points. A test on approx. 50.000 data-points has 
	# show that this is the case for ca. 0,02 per cent of the points. So I 
	# have to check the data anyway manually to get rid of these points.
	# I make pictures of each data-set that shall help me with that.
	print(data)
	print(data_to_be_written)
	plot_data(data, data_to_be_written, plotfile)

	#Finally, make a file with 1 and 0's that outlines where noise was detected.


	
	#write_to_file(outfile, data_to_be_written)


# The very first file will NOT undergo the noise reducing algorithm.
# See the definitions and comments in the above functions why this is the case.
# Anyhow, of course this file is needed in the same format as the other noise 
# reduced files (JUST the table with the values) and with a proper filename 
# ("0001_noise_reduced.txt") in the correct folders.
# But the raw analyzed file has additional data above the table and the 
# filename is not correct. I take care of this here.

def make_binary_noisefiles(original_data, noise_reduced_data):
	binarynoise=[]

	data=original_data
	data_to_be_written=noise_reduced_data
	
	if type(data) is dict:
		data=transform_dict_to_list(data)
	
	if type(data_to_be_written) is dict:
		data_to_be_written=transform_dict_to_list(data_to_be_written)	
	
	for i in range(len(data)):
		if abs(data[i][1]-data_to_be_written[i][1])<1.0:
			binarynoise.append((data[i][0],1))
		else:
			binarynoise.append((data[i][0],0))

	
	write_to_file(binarynoisefile, binarynoise)


def make_correct_first_file(file_location, name_base, minimum_number):
	first_file_addendum = name_base+(str(minimum_number-1)).zfill(4)+'_Lower.txt'   #'0001_Lower.txt'
	first_infile = os.path.join(file_location, first_file_addendum)
	first_outfile = os.path.join(file_location, (str(minimum_number-1)).zfill(4)+'_noise_reduced.txt')#'0001_noise_reduced.txt')

	data = []
	# Don't add everything, JUST add sth. if it belongs to the table 
	# of values.
	add = False

	# Open the first file and extract the data.
	with open(first_infile, 'r') as f:
		for line in f:
			if 'Length' in line or 'microstrain' in line \
						or '(deg C)' in line or 'Spectral' in line:
				add = True
			elif add:
				data.append(line.strip())

	# Now write this to the correct outfile.
	with open(first_outfile, 'w') as f:
		for element in data:
			f.write('%s\n' % element)


# A function that does everything to move all the files into the
# correct folders.
# ATTENTION: It is assumed that these folders exist.
def move_files(file_location):
	# The raw analysis files are located in file_location.
	# However, this is just a subfolder and I need to move all the noise
	# reduced files into other folders that are "one level above" the
	# RAW-folder.
	folder_base = file_location.split('RAW')[0]

	# Under windows " \ " is used instead of " / " to separate subfolders
	# in the file location name. 
	# This gave me major headaches how to account for that since " \ " has a 
	# special meaning in strings under python.
	# However, doing it this way I managed to get things done.
	# Especially os.path.join() is very useful to get operative system 
	# dependend seperators correct.
	all_folders = os.listdir(folder_base)

	for folder_name in all_folders:
		if 'Noise_reduced_before_manual_correction' in folder_name:
			first_folder = os.path.join(folder_base, folder_name)
		elif 'Noise_reduced_after_manual_correction' in folder_name:
			second_folder = os.path.join(folder_base, folder_name)
		elif 'PNG' in folder_name:
			png_folder = os.path.join(folder_base, folder_name)
		elif 'Binary_noise_files' in folder_name:
			binary_files = os.path.join(folder_base, folder_name)		

	all_sorted_filenames = sorted(os.listdir(file_location))

	for filename in all_sorted_filenames:
		if "_noise_reduced" in filename:
			from_here = os.path.join(file_location, filename)
			to_there_1 = os.path.join(first_folder, filename)
			to_there_2 = os.path.join(second_folder, filename)
			shutil.copy2(from_here, to_there_1)
			shutil.move(from_here, to_there_2)
		elif "_binary_noise" in filename:
			from_here = os.path.join(file_location, filename)
			to_there = os.path.join(binary_files, filename)
			shutil.move(from_here, to_there)			
		# Since I never know if the filename ending is upper or lower case when
		# these files are created, I just convert the filename to all caps.
		elif ".PNG" in filename.upper():
			from_here = os.path.join(file_location, filename)
			to_there = os.path.join(png_folder, filename)
			shutil.move(from_here, to_there)





## ## ## ## ## Here the execution of the program starts. ## ## ## ## ##



# That I can call main() from other modules with the correct parameters 
# I decided to have file_location, name_base and maximum_number as parameters.
# 
# To have this however also as a standalone program, I ask for these 
# parameters in the if-construct that is called when this program is called
# on the shell.
def main(file_location, name_base, minimum_number, maximum_number, maximum_difference_between_points, maximum_difference_for_interpolation, maxallowedstrainincrease):


	# ATTENTION: DON'T CHECK THE FIRST FILE!!! 
	# IT DOESN'T HAVE A PREVIOUS MEASUREMENT FILE AND THAT WILL CAUSE ERRORS!
	for i in range(minimum_number, (maximum_number + 1)):
		print("noise reducing file %s"%i)
		# Just generate for each file the correct filenames to work with.
		# The above mentioned OBR measurement/analysis program sets
		# the word "_Lower" automatically at the end of each filename.
		# If OBR_analyzer_simple.py program was used to automatically press
		# the buttons for analysis, the file with the second to last measurement
		# as reference file automatically has "_older_reference_Lower" at the 
		# end of the filename.
		# 
		# # zfill() adds zeros in front of the given string. 
		# i is a number, not a string. Thus I use str() to convert it to a 
		# string first.
		# 
		# Thanks to windows I need to jump through hoops to get the 
		# proper filenames.
		infile_addendum = name_base + str(i).zfill(4) + '_Lower.txt'
		infile = os.path.join(file_location, infile_addendum)
		outfile_addendum = str(i).zfill(4) + '_noise_reduced.txt'
		outfile = os.path.join(file_location, outfile_addendum)
		
		binarynoisefile_addendum = str(i).zfill(4) + '_binary_noise.txt'
		binarynoisefile = os.path.join(file_location, binarynoisefile_addendum)
		
		other_reference_measurement_addendum = name_base + \
								str(i).zfill(4) + '_Lower_summed.txt'
		other_reference_measurement = os.path.join(file_location, \
											other_reference_measurement_addendum)
		previous_measurement_addendum = name_base + str((i-1)).zfill(4) + '_Lower.txt'
		
		previous_measurement = os.path.join(file_location, previous_measurement_addendum)
		plotfile_addendum = str(i).zfill(4) + '_plot.png'
		plotfile = os.path.join(file_location, plotfile_addendum)

		# Here the actual noise reduction takes place, see the 
		# function-definition(s) above.
		reduce_noise(infile, outfile, binarynoisefile, other_reference_measurement, previous_measurement, plotfile, maximum_difference_between_points, maximum_difference_for_interpolation, maxallowedstrainincrease)

	# When all files are corrected for outliers, ...
	# 
	# ... make a proper first file 
	# (see comment to make_correct_first_file()) ...
	make_correct_first_file(file_location, name_base, minimum_number)
	# ... and move the files into the correct folders.
	move_files(file_location)




def Difference_over_cycles(ListofMeasurements,Length,Allfilesdict):
	
	#Given a list of measurements, typically at different loadlevels such as 13,23,33... then give give a Strain for each measurement on a given length given the full measurementdicitonary. 
	
	StrainAtLength=[]
	for measurement in ListofMeasurements:
		measurementdict=Allfilesdict[measurement]
		StrainAtLength.append(measurementdict[find_nearest(measurementdict, Length)])
	
	return StrainAtLength


def Noise_Over_One_Measurement(NumberofCycles,StartOfCycles,EndOfCycles,file_location,name_base,MaxDifference):

	allfilesMDiffDict={}
	allfilesMDiffDictNoise={}
	allfilesMDiffDictonlyNoise={}
	
	for i in range(NumberOfCycles):	
		
		#This loop goes thorugh one load ramping at the time and makes noise files based on the current measurement.
		
		maximum_number = int(EndOfCycles[i])     #Measurement from end of load ramping
		minimum_number = int(StartOfCycles[i])   #Measurement from start of load ramping
		
		for d in range(minimum_number,maximum_number+1):
			
			#This loop goes through all measurements in one ramping
	
			infile=os.path.join(file_location,name_base+(str(d)).zfill(4)+'_Lower.txt')
			pastinfile=os.path.join(file_location,name_base+(str(d-1)).zfill(4)+'_Lower.txt')
			
			onefiledict=data_from_one_file(infile)      	#Current file as a dictionary with length: strain sa entries. 
			pastfiledict=data_from_one_file(pastinfile) 	#The past file as the same.
			
			StrainDiffDict={}
			StrainDiffNoiseDict={}
			StrainDiffonlyNoiseDict={}
			
			for key, value in onefiledict.items():     	#Goes through the current file and picks out the key (Length) and the value (Strain).

				if abs(value)>MaxDifference:          	#Makes a dictionary of binary noise along the length.
					StrainDiffNoiseDict[key]=1.0 
					StrainDiffonlyNoiseDict[key]=1.0
					
				else:
					StrainDiffNoiseDict[key]=0.0				
			
			allfilesMDiffDict[d]= StrainDiffDict          #Actual difference
			allfilesMDiffDictNoise[d]=StrainDiffNoiseDict #Binary difference	
			allfilesMDiffDictonlyNoise[d]=StrainDiffonlyNoiseDict #All measurements with only noise.
			
	return allfilesMDiffDict, allfilesMDiffDictNoise, allfilesMDiffDictonlyNoise


def Noise_Over_Load_Steps(NumberofCycles,StartOfCycles,EndOfCycles,file_location,name_base,MaxDifferenceBetweenLoadSteps):

	allfilesDispDiffDict={}
	allfilesDispDiffDictNoise={}
	allfilesDispDiffDictonlyNoise={}
	
	for i in range(NumberOfCycles):	
		
		#This loop goes thorugh one load ramping at the time and makes noise files based on the current measurement.
		
		maximum_number = int(EndOfCycles[i]) #Measurement from end of load ramping
		minimum_number = int(StartOfCycles[i])     #Measurement from start of load ramping
		
		for d in range(minimum_number,maximum_number+1):
			
			#This loop goes through all measurements in one ramping
	
			infile=os.path.join(file_location,name_base+(str(d)).zfill(4)+'_Lower.txt')
			pastinfile=os.path.join(file_location,name_base+(str(d-1)).zfill(4)+'_Lower.txt')
			
			onefiledict=data_from_one_file(infile)      	#Current file as a dictionary with length: strain sa entries. 
			pastfiledict=data_from_one_file(pastinfile) 	#The past file as the same.
			
			StrainDiffDict={}
			StrainDiffNoiseDict={}
			StrainDiffonlyNoiseDict={}
			
			for key, value in onefiledict.items():     	#Goes through the current file and picks out the key (Length) and the value (Strain).
				StrainDiff=abs(value)-abs(pastfiledict[key]) 	#Establishes the difference from the past file.
				StrainDiffDict[key]=StrainDiff     	#Appends the difference to a dictionarry equal to the one in the current file. 
				
				if StrainDiff>MaxDifferenceBetweenLoadSteps:          	#Makes a dictionary of binary noise along the length.
					StrainDiffNoiseDict[key]=1.0 
					StrainDiffonlyNoiseDict[key]=1.0
					
				else:
					StrainDiffNoiseDict[key]=0.0				
			
			#Appends the noise files to dictionaries across measurements.
			
			allfilesDispDiffDict[d]= StrainDiffDict          #Actual difference
			allfilesDispDiffDictNoise[d]=StrainDiffNoiseDict #Binary difference	
			allfilesDispDiffDictonlyNoise[d]=StrainDiffonlyNoiseDict #All measurements with only noise.
			
	return allfilesDispDiffDict, allfilesDispDiffDictNoise, allfilesDispDiffDictonlyNoise
			


def Noise_Over_Cycles(NumberofCycles,StartOfCycles,EndOfCycles,file_location,name_base,MaxDifferenceBetweenLoadSteps):

	allfilesNDiffDict={}
	allfilesNDiffDictNoise={}
	allfilesNDiffDictonlyNoise={}
	
	for d in range(NumberOfLoadSteps):
		#Goes through each measurement (measurement 1 at 100 cycles and 200 cycles....)
		for a in StartOfCycles: # takes the startmeasurement at each cycle.
			StrainDiffDict={}
			StrainDiffNoiseDict={}
			StrainDiffonlyNoiseDict={}
			num=int(float(d)+float(a)) #So its number of measurement (say d=0) and the start measurement (say a is 15)...
			for key, value in allfilesdict[num].items():
				if a==StartOfCycles[0]: #If first cycle just floor the difference
					StrainDiff=value-value 
					StrainDiffDict[key]=StrainDiff 
				else: 
					StrainDiff=abs(value)-abs(allfilesdict[num-NumberOfLoadSteps-1][key])
					StrainDiffDict[key]=StrainDiff
					
				if StrainDiff>MaxDifferenceBetweenCycles:
					StrainDiffNoiseDict[key]=1.0
					StrainDiffonlyNoiseDict[key]=1.0
				else:
					StrainDiffNoiseDict[key]=0.0	
					
				allfilesNDiffDict[num]= StrainDiffDict
				allfilesNDiffDictNoise[num]=StrainDiffNoiseDict
				allfilesNDiffDictonlyNoise[num]=StrainDiffonlyNoiseDict
				
	return allfilesNDiffDict, allfilesNDiffDictNoise, allfilesNDiffDictonlyNoise

def Measurement_Dictionary(NumberofCycles,StartOfCycles,EndOfCycles,file_location,name_base,MaxDifferenceBetweenLoadSteps):

	allfilesdict={}
	
	for i in range(NumberOfCycles):	
		
		#This loop goes thorugh one load ramping at the time and makes noise files based on the current measurement.
		
		maximum_number = int(EndOfCycles[i]) #Measurement from end of load ramping
		minimum_number = int(StartOfCycles[i])     #Measurement from start of load ramping
		
		for d in range(minimum_number,maximum_number+1):
			
			#This loop goes through all measurements in one ramping
			
			infile=os.path.join(file_location,name_base+(str(d)).zfill(4)+'_Lower.txt')
			onefiledict=data_from_one_file(infile)      	#Current file as a dictionary with length: strain sa entries. 
			allfilesdict[d] = onefiledict			 #Measurement file
	
	return allfilesdict

def Summed_Measurement_Dictionary(StartOfCycles,EndOfCycles,file_location,name_base):
	allsummedfilesdict={}
	
	for i in range(0,len(EndOfCycles)):	
		#Appends the summed strain into a dicitonary (note; '_Lower_summed.txt' in infile)
		maximum_number = int(EndOfCycles[i])
		minimum_number = int(StartOfCycles[i])
		
		for d in range(minimum_number,maximum_number+1):
	
			infile=os.path.join(file_location,name_base+(str(d)).zfill(4)+'_Lower_summed.txt')
			
			onefiledict=data_from_one_file(infile)
			allsummedfilesdict[d] = onefiledict	
	
	return allsummedfilesdict

def Combined_Noise(allfilesNDiffDictonlyNoise,allfilesDispDiffDictonlyNoise,allfilesMDiffDictonlyNoise):
	
	


	BinaryNoiseDict={}
	for key, value in allfilesNDiffDictonlyNoise.items():
		if key in allfilesDispDiffDictonlyNoise: #Check if its noise in measurement in both noisedicts
			Mkey=key

			Dictionary=value
			BinaryNoiseDictatM={}
			for key, value in Dictionary.items(): #Check if its noise in length in both noisedicts
				if key in allfilesDispDiffDictonlyNoise[Mkey]:
					BinaryNoiseDictatM[key]=value
		BinaryNoiseDict[Mkey]=BinaryNoiseDictatM
	
	print(BinaryNoiseDict[13])	
	
	#return(BinaryNoiseDict)

	BinaryNoiseDictUpdated=deepcopy(BinaryNoiseDict)
	
	for key, value in allfilesMDiffDictonlyNoise.items():
		
		DictToBeUpdated=BinaryNoiseDict[key]
		
		DictToBeUpdated.update(value)
		
		BinaryNoiseDictUpdated[key]=DictToBeUpdated
		
	print(BinaryNoiseDictUpdated[13])	
	return BinaryNoiseDictUpdated

def Reduce_Noise(BinaryNoiseDict,FirstMeasurementinFirstCycle,allfilesdict):

	allfilesnoisefree=deepcopy(allfilesdict)
	allfileswithnoise=deepcopy(allfilesdict)	

	for key in sorted(BinaryNoiseDict.keys()):#  BinaryNoiseDict.items():
		Mkey=key
		
		Dictionary=BinaryNoiseDict[Mkey]
		for Lkey, value in Dictionary.items(): #Check if its noise in length in both noisedicts
			for i in [10,20,30,40,50,60,70,80,90,100,110,120,130,140,160,170,180]:
				if Mkey-i>FirstMeasurementinFirstCycle:
					if Lkey not in BinaryNoiseDict[Mkey-i]:
						noisefreereading=allfilesdict[Mkey-i][Lkey]
						allfilesnoisefree[Mkey][Lkey]=noisefreereading

						break
	return allfilesnoisefree,allfileswithnoise

def PrepareDictionaryforPlotting(TypicalLengthList,PlotCycles,MeasurementDictionary):
	
	MeasurementsAtPlotCycles=[]
	PlotLength=[]
	
	for point in TypicalLengthList:
		
		Length=point[0]
		StrainAtLength=Difference_over_cycles(PlotCycles,Length,MeasurementDictionary)
		MeasurementsAtPlotCycles.append(StrainAtLength)
		PlotLength.append(Length)
	
	return MeasurementsAtPlotCycles, PlotLength

if __name__ == '__main__':
	
	'''
	------------------------------------------------------------------------------ Start Input --------------------------------------------------------------------------------------------
	'''
	
	NumberOfCycles=17
	NumberOfLoadSteps=9 #Say 4 is zeroreading and 5 is first and 13 is last, then: 5 = 1 cycle no. 1, 6 = cycle no. 2, 7 = cycle no. 3 ,...., 13 = cycle no. 9
	FirstMeasurementinFirstCycle=5
	LastMeasurementInFirstCycle=13
	zero_file=4
	GapMeasurement=1
	
	PlotCycle=10
	
	MaxDifferenceBetweenCycles=250
	MaxDifferenceBetweenLoadSteps=250
	MaxDifference=3000
	
	file_location = r'C:\Users\eivinhug\OneDrive - NTNU\PhD\Testing\Laminate_E\OBR_Files\E01_Fatigue_2_28082018\RAW' 
	name_base = 'E01_fibre 1_' 	
	
	'''
	------------------------------------------------------------------ End Input, intitate pre-processes ------------------------------------------------------------------------------------
	'''
	
	zerofile=os.path.join(file_location,name_base+(str(zero_file)).zfill(4)+'_Lower.txt')
	
	StartOfCycles=np.arange(FirstMeasurementinFirstCycle,((NumberOfLoadSteps+GapMeasurement)*(NumberOfCycles-1))+FirstMeasurementinFirstCycle+1,NumberOfLoadSteps+1)

	EndOfCycles=np.arange(LastMeasurementInFirstCycle,((NumberOfLoadSteps+GapMeasurement)*(NumberOfCycles-1))+LastMeasurementInFirstCycle+1,NumberOfLoadSteps+1)
	
	PlotCycles=np.arange(PlotCycle,((NumberOfLoadSteps+GapMeasurement)*(NumberOfCycles-1))+PlotCycle+1,NumberOfLoadSteps+1)

	'''
	----------------------------------------------------------------- End pre-processes, intitate modules for establishing dictionaries ------------------------------------------------------------------------------------
	'''	
	
	allfilesdict=Measurement_Dictionary(NumberOfCycles,StartOfCycles,EndOfCycles,file_location,name_base,MaxDifferenceBetweenLoadSteps)
	
	allsummedfilesdict = Summed_Measurement_Dictionary(StartOfCycles,EndOfCycles,file_location,name_base)	
	
	Datafromzerofile=data_from_one_file(zerofile)
	
	TypicalLengthList=transform_dict_to_list(Datafromzerofile)
	
	'''
	---------------------------------------------------------------------------- Established dictionary of all files ------------------------------------------------------------------------------------
	'''		
	
	allfilesDispDiffDict, allfilesDispDiffDictNoise, allfilesDispDiffDictonlyNoise=Noise_Over_Load_Steps(NumberOfCycles,StartOfCycles,EndOfCycles,file_location,name_base,MaxDifferenceBetweenLoadSteps)
	
	allfilesNDiffDict, allfilesNDiffDictNoise, allfilesNDiffDictonlyNoise=Noise_Over_Cycles(NumberOfCycles,StartOfCycles,EndOfCycles,file_location,name_base,MaxDifferenceBetweenLoadSteps)	
	
	allfilesMDiffDict, allfilesMDiffDictNoise, allfilesMDiffDictonlyNoise=Noise_Over_One_Measurement(NumberOfCycles,StartOfCycles,EndOfCycles,file_location,name_base,MaxDifference)
	
	'''
	------------------------------------------------------------ For this to work smoothly, the first cycle of measurements should be smoothed ------------------------------------------------------------------------------------
	'''		
	
	#FirstMeasurement = SmoothenOverLength(allfilesdict,MeasurementsToSmoothen,allfilesMDiffDictonlyNoise)
	
	'''
	--------------------------------------------------------------------- Established dictionary of what readings has noise ------------------------------------------------------------------------------------
	'''	
	
	BinaryNoiseDict=Combined_Noise(allfilesNDiffDictonlyNoise,allfilesDispDiffDictonlyNoise,allfilesMDiffDictonlyNoise)

	'''
	--------------------------------------------------------------------------- Established dictionaries, intitate noise reduction ---------------------------------------------------------------------------------------
	'''	
	
	allfilesnoisefree,allfileswithnoise=Reduce_Noise(BinaryNoiseDict,FirstMeasurementinFirstCycle,allfilesdict)
	
	'''
	----------------------------------------------------------------------------------- End noise reduction, start plotting -------------------------------------------------------------------------------------------------------
	'''
	fig = plt.figure(figsize=plt.figaspect(0.5))
	
	PlotDict=PrepareDictionaryforPlotting(TypicalLengthList,PlotCycles,allfileswithnoise)
	
	X1,Y1=np.meshgrid(PlotCycles,PlotDict[1])
	Z1=np.array(PlotDict[0])
	ax = fig.add_subplot(1, 2, 1, projection='3d')
	ax.plot_surface(X1, Y1, Z1,cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)	
	
	PlotDict=PrepareDictionaryforPlotting(TypicalLengthList,PlotCycles,allfilesnoisefree)
	
	X1,Y1=np.meshgrid(PlotCycles,PlotDict[1])
	Z1=np.array(PlotDict[0])
	ax = fig.add_subplot(1, 2, 2, projection='3d')	
	ax.plot_surface(X1, Y1, Z1,cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)		
	
	plt.show()


	'''
	------------------------------------------------------------------------------------------------- Ended plotting -------------------------------------------------------------------------------------------------------
	'''




